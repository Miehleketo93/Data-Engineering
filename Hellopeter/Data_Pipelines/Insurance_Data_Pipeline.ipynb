{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb94a147",
   "metadata": {},
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab9364",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Environment Setup\n",
    "Install all required libraries for the data collection pipeline.\n",
    "This cell only needs to be run once per environment.\n",
    "\"\"\"\n",
    "#!pip install -q aiohttp aiofiles pandas tqdm psutil requests beautifulsoup4 scrapy spacy nltk transformers datasets\n",
    "\n",
    "print(\" Ready to import libraries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac6314",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Import Required Libraries\n",
    "Import all necessary libraries for asynchronous API calls, data processing,\n",
    "and checkpoint management.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from tqdm.asyncio import tqdm\n",
    "import sys\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "print(\" All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff7235",
   "metadata": {},
   "source": [
    "## Configuration and Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190dedf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Configuration Settings\n",
    "Configure all pipeline parameters including rate limits, retry settings,\n",
    "and file paths. Adjust these based on your needs and API limits.\n",
    "\"\"\"\n",
    "\n",
    "# ============= CONFIGURATION =============\n",
    "CONFIG = {\n",
    "    # Rate limiting (requests per second)\n",
    "    'MAX_CONCURRENT_REQUESTS': 3,  # Reduced for 8GB RAM\n",
    "    'REQUEST_DELAY': 0.5,  # Delay between requests in seconds\n",
    "    \n",
    "    # Retry settings\n",
    "    'MAX_RETRIES': 5,\n",
    "    'INITIAL_BACKOFF': 2,  # Initial backoff in seconds\n",
    "    'MAX_BACKOFF': 60,  # Maximum backoff in seconds\n",
    "    \n",
    "    # Memory management\n",
    "    'CHUNK_SIZE': 100,  # Save to disk every N records\n",
    "    'BATCH_SIZE': 50,  # Process N pages before clearing memory\n",
    "    \n",
    "    # File paths\n",
    "    'CHECKPOINT_FILE': 'pipeline_checkpoint.json',\n",
    "    'LOG_FILE': 'pipeline_execution.log',\n",
    "    'ERROR_LOG': 'pipeline_errors.log',\n",
    "    'TEMP_DIR': 'temp_data',\n",
    "    'FINAL_OUTPUT': 'customer_insurance_reviews_final.csv',\n",
    "}\n",
    "\n",
    "# Create necessary directories\n",
    "Path(CONFIG['TEMP_DIR']).mkdir(exist_ok=True)\n",
    "\n",
    "# ============= LOGGING SETUP =============\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(CONFIG['LOG_FILE']),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Separate error logger\n",
    "error_logger = logging.getLogger('errors')\n",
    "error_handler = logging.FileHandler(CONFIG['ERROR_LOG'])\n",
    "error_handler.setLevel(logging.ERROR)\n",
    "error_logger.addHandler(error_handler)\n",
    "\n",
    "logger.info(\" Configuration and logging initialized\")\n",
    "print(f\" Configuration loaded - Max concurrent requests: {CONFIG['MAX_CONCURRENT_REQUESTS']}\")\n",
    "print(f\" Checkpoint file: {CONFIG['CHECKPOINT_FILE']}\")\n",
    "print(f\" Temporary data directory: {CONFIG['TEMP_DIR']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c695fc",
   "metadata": {},
   "source": [
    "## API Endpoints Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## API Endpoints Configuration\n",
    "Define all insurance company APIs to be scraped. Each entry contains\n",
    "the API URL and a business identifier.\n",
    "\"\"\"\n",
    "from config import INSURANCE_APIS  # API list imported from config.py\n",
    "\n",
    "\n",
    "logger.info(f\" Configured {len(INSURANCE_APIS)} insurance APIs\")\n",
    "print(f\" Total APIs to process: {len(INSURANCE_APIS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f1ea91",
   "metadata": {},
   "source": [
    "## Checkpoint Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e831500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Checkpoint Management\n",
    "Functions to save and load pipeline progress. This allows the pipeline\n",
    "to resume from where it left off if interrupted.\n",
    "\"\"\"\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"Manages pipeline checkpoint state for resume capability\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_file: str):\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "        self.state = self._load_checkpoint()\n",
    "    \n",
    "    def _load_checkpoint(self) -> Dict:\n",
    "        \"\"\"Load checkpoint from file or create new\"\"\"\n",
    "        if Path(self.checkpoint_file).exists():\n",
    "            try:\n",
    "                with open(self.checkpoint_file, 'r') as f:\n",
    "                    state = json.load(f)\n",
    "                logger.info(f\" Loaded checkpoint from {self.checkpoint_file}\")\n",
    "                return state\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load checkpoint: {e}. Starting fresh.\")\n",
    "                return self._create_empty_checkpoint()\n",
    "        return self._create_empty_checkpoint()\n",
    "    \n",
    "    def _create_empty_checkpoint(self) -> Dict:\n",
    "        \"\"\"Create empty checkpoint structure\"\"\"\n",
    "        return {\n",
    "            'current_api_index': 0,\n",
    "            'completed_apis': [],\n",
    "            'failed_apis': [],\n",
    "            'last_update': None,\n",
    "            'total_records_processed': 0,\n",
    "            'start_time': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        \"\"\"Save current state to file\"\"\"\n",
    "        self.state['last_update'] = datetime.now().isoformat()\n",
    "        try:\n",
    "            with open(self.checkpoint_file, 'w') as f:\n",
    "                json.dump(self.state, f, indent=2)\n",
    "            logger.debug(f\"Checkpoint saved: API {self.state['current_api_index']}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save checkpoint: {e}\")\n",
    "    \n",
    "    def mark_api_complete(self, api_name: str, records_count: int):\n",
    "        \"\"\"Mark an API as completed\"\"\"\n",
    "        self.state['completed_apis'].append({\n",
    "            'name': api_name,\n",
    "            'records': records_count,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        self.state['current_api_index'] += 1\n",
    "        self.state['total_records_processed'] += records_count\n",
    "        self.save_checkpoint()\n",
    "    \n",
    "    def mark_api_failed(self, api_name: str, error: str):\n",
    "        \"\"\"Mark an API as failed\"\"\"\n",
    "        self.state['failed_apis'].append({\n",
    "            'name': api_name,\n",
    "            'error': str(error),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        self.state['current_api_index'] += 1\n",
    "        self.save_checkpoint()\n",
    "    \n",
    "    def get_resume_index(self) -> int:\n",
    "        \"\"\"Get the index to resume from\"\"\"\n",
    "        return self.state['current_api_index']\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset checkpoint to start fresh\"\"\"\n",
    "        self.state = self._create_empty_checkpoint()\n",
    "        self.save_checkpoint()\n",
    "        logger.info(\"Checkpoint reset\")\n",
    "\n",
    "# Initialize checkpoint manager\n",
    "checkpoint = CheckpointManager(CONFIG['CHECKPOINT_FILE'])\n",
    "logger.info(\" Checkpoint manager initialized\")\n",
    "print(f\" Will resume from API index: {checkpoint.get_resume_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f1375",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## API Diagnostic Tool\n",
    "Test a single API endpoint to verify it's working correctly.\n",
    "Run this BEFORE the full pipeline to diagnose issues!\n",
    "\"\"\"\n",
    "\n",
    "async def test_single_api(business_url: str, business_name: str):\n",
    "    \"\"\"\n",
    "    Test a single API endpoint with detailed diagnostics\n",
    "    \n",
    "    Args:\n",
    "        business_url: API endpoint to test\n",
    "        business_name: Business identifier\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TESTING API: {business_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"URL: {business_url}\\n\")\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "    }\n",
    "    \n",
    "    connector = aiohttp.TCPConnector(limit=1)\n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        try:\n",
    "            print(\" Making request...\")\n",
    "            async with session.get(business_url, headers=headers, timeout=aiohttp.ClientTimeout(total=10)) as response:\n",
    "                status = response.status\n",
    "                print(f\" Status Code: {status}\")\n",
    "                print(f\" Headers: {dict(response.headers)}\\n\")\n",
    "                \n",
    "                if status == 200:\n",
    "                    data = await response.json()\n",
    "                    print(\" SUCCESS! API returned data\")\n",
    "                    print(f\"\\n Response Structure:\")\n",
    "                    print(f\"   Keys: {list(data.keys())}\")\n",
    "                    \n",
    "                    if 'data' in data:\n",
    "                        print(f\"   Data records: {len(data.get('data', []))}\")\n",
    "                        if data['data']:\n",
    "                            print(f\"   Sample record keys: {list(data['data'][0].keys())}\")\n",
    "                    \n",
    "                    if 'last_page' in data:\n",
    "                        print(f\"   Total pages: {data.get('last_page')}\")\n",
    "                    \n",
    "                    return True, data\n",
    "                \n",
    "                elif status == 202:\n",
    "                    text = await response.text()\n",
    "                    print(f\"  HTTP 202 (Accepted) - API is queuing requests\")\n",
    "                    print(f\"   This usually means authentication required or rate limiting\")\n",
    "                    print(f\"   Response: {text[:200]}\")\n",
    "                    return False, None\n",
    "                \n",
    "                else:\n",
    "                    text = await response.text()\n",
    "                    print(f\" HTTP {status}\")\n",
    "                    print(f\"   Response: {text[:500]}\")\n",
    "                    return False, None\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\" ERROR: {str(e)}\")\n",
    "            return False, None\n",
    "\n",
    "# Test with one API\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"API DIAGNOSTICS - Run this FIRST!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nThis will test if the HelloPeter API is accessible.\")\n",
    "print(\"Run the cell below to test:\\n\")\n",
    "print(\"  await test_single_api(\")\n",
    "print(\"  )\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea92240b",
   "metadata": {},
   "source": [
    "## Rate Limiter Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc761f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Rate Limiter\n",
    "Implements token bucket algorithm for rate limiting API requests.\n",
    "Prevents overwhelming the API and ensures compliance with rate limits.\n",
    "\"\"\"\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Token bucket rate limiter for API requests\"\"\"\n",
    "    \n",
    "    def __init__(self, max_requests: int, delay: float):\n",
    "        self.max_requests = max_requests\n",
    "        self.delay = delay\n",
    "        self.semaphore = asyncio.Semaphore(max_requests)\n",
    "        self.last_request_time = 0\n",
    "    \n",
    "    async def acquire(self):\n",
    "        \"\"\"Acquire permission to make a request\"\"\"\n",
    "        async with self.semaphore:\n",
    "            # Ensure minimum delay between requests\n",
    "            current_time = time.time()\n",
    "            time_since_last = current_time - self.last_request_time\n",
    "            if time_since_last < self.delay:\n",
    "                await asyncio.sleep(self.delay - time_since_last)\n",
    "            self.last_request_time = time.time()\n",
    "\n",
    "rate_limiter = RateLimiter(\n",
    "    CONFIG['MAX_CONCURRENT_REQUESTS'],\n",
    "    CONFIG['REQUEST_DELAY']\n",
    ")\n",
    "logger.info(\" Rate limiter initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac4cb7f",
   "metadata": {},
   "source": [
    "## Async HTTP Fetcher with Retry Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce718f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Async HTTP Fetcher\n",
    "Asynchronous HTTP request handler with exponential backoff retry logic.\n",
    "Handles network errors, timeouts, and rate limiting gracefully.\n",
    "\"\"\"\n",
    "\n",
    "async def fetch_with_retry(\n",
    "    session: aiohttp.ClientSession,\n",
    "    url: str,\n",
    "    business_name: str,\n",
    "    attempt: int = 0\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch data from API with exponential backoff retry logic\n",
    "    \n",
    "    Args:\n",
    "        session: aiohttp client session\n",
    "        url: API endpoint URL\n",
    "        business_name: Business identifier for logging\n",
    "        attempt: Current retry attempt number\n",
    "    \n",
    "    Returns:\n",
    "        JSON response as dictionary or None if failed\n",
    "    \"\"\"\n",
    "    if attempt >= CONFIG['MAX_RETRIES']:\n",
    "        error_msg = f\"Max retries ({CONFIG['MAX_RETRIES']}) reached for {business_name}\"\n",
    "        logger.error(error_msg)\n",
    "        error_logger.error(f\"{business_name} | {url} | {error_msg}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Apply rate limiting\n",
    "        await rate_limiter.acquire()\n",
    "        \n",
    "        # Add headers to mimic browser request\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "            'Accept': 'application/json',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "        }\n",
    "        \n",
    "        # Make the request with timeout\n",
    "        async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=30)) as response:\n",
    "            # HelloPeter API returns 202 with valid data - accept both 200 and 202\n",
    "            if response.status == 200 or response.status == 202:\n",
    "                try:\n",
    "                    data = await response.json()\n",
    "                    # Verify we got actual data, not just an acknowledgment\n",
    "                    if data and isinstance(data, dict):\n",
    "                        return data\n",
    "                    else:\n",
    "                        logger.warning(f\"Empty or invalid response for {business_name}\")\n",
    "                        return None\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to parse JSON for {business_name}: {e}\")\n",
    "                    return None\n",
    "            elif response.status == 429:  # Too many requests\n",
    "                backoff = min(CONFIG['MAX_BACKOFF'], CONFIG['INITIAL_BACKOFF'] * (2 ** attempt))\n",
    "                logger.warning(f\"Rate limited for {business_name}. Retrying in {backoff}s (attempt {attempt + 1}/{CONFIG['MAX_RETRIES']})\")\n",
    "                await asyncio.sleep(backoff)\n",
    "                return await fetch_with_retry(session, url, business_name, attempt + 1)\n",
    "            elif response.status == 404:\n",
    "                logger.error(f\"API endpoint not found (404) for {business_name}: {url}\")\n",
    "                return None\n",
    "            else:\n",
    "                logger.warning(f\"HTTP {response.status} for {business_name}\")\n",
    "                return None\n",
    "    \n",
    "    except asyncio.TimeoutError:\n",
    "        backoff = min(CONFIG['MAX_BACKOFF'], CONFIG['INITIAL_BACKOFF'] * (2 ** attempt))\n",
    "        logger.warning(f\"Timeout for {business_name}. Retrying in {backoff}s (attempt {attempt + 1}/{CONFIG['MAX_RETRIES']})\")\n",
    "        await asyncio.sleep(backoff)\n",
    "        return await fetch_with_retry(session, url, business_name, attempt + 1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        backoff = min(CONFIG['MAX_BACKOFF'], CONFIG['INITIAL_BACKOFF'] * (2 ** attempt))\n",
    "        logger.warning(f\"Error for {business_name}: {str(e)}. Retrying in {backoff}s (attempt {attempt + 1}/{CONFIG['MAX_RETRIES']})\")\n",
    "        error_logger.error(f\"{business_name} | {url} | {str(e)}\")\n",
    "        await asyncio.sleep(backoff)\n",
    "        return await fetch_with_retry(session, url, business_name, attempt + 1)\n",
    "\n",
    "logger.info(\"âœ“ Async fetcher with retry logic defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc069d",
   "metadata": {},
   "source": [
    "## Data Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee5b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Data Storage Functions\n",
    "Functions to save data to disk in chunks to minimize memory usage.\n",
    "Uses JSON format for temporary storage and CSV for final output.\n",
    "\"\"\"\n",
    "\n",
    "async def save_chunk_to_disk(data: List[Dict], business_name: str, chunk_id: int):\n",
    "    \"\"\"\n",
    "    Save a chunk of data to temporary JSON file\n",
    "    \n",
    "    Args:\n",
    "        data: List of review records\n",
    "        business_name: Business identifier\n",
    "        chunk_id: Unique chunk identifier\n",
    "    \"\"\"\n",
    "    filename = Path(CONFIG['TEMP_DIR']) / f\"{business_name}_chunk_{chunk_id}.json\"\n",
    "    try:\n",
    "        async with aiofiles.open(filename, 'w') as f:\n",
    "            await f.write(json.dumps(data))\n",
    "        logger.debug(f\"Saved chunk {chunk_id} for {business_name} ({len(data)} records)\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save chunk {chunk_id} for {business_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_temp_files_for_business(business_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all temporary chunks for a business and return as DataFrame\n",
    "    \n",
    "    Args:\n",
    "        business_name: Business identifier\n",
    "    \n",
    "    Returns:\n",
    "        Combined DataFrame of all chunks\n",
    "    \"\"\"\n",
    "    temp_dir = Path(CONFIG['TEMP_DIR'])\n",
    "    pattern = f\"{business_name}_chunk_*.json\"\n",
    "    chunk_files = sorted(temp_dir.glob(pattern))\n",
    "    \n",
    "    if not chunk_files:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    all_data = []\n",
    "    for chunk_file in chunk_files:\n",
    "        try:\n",
    "            with open(chunk_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                all_data.extend(data)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load {chunk_file}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    df['Business_Name'] = business_name\n",
    "    return df\n",
    "\n",
    "logger.info(\" Data storage functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1fd2e5",
   "metadata": {},
   "source": [
    "## Single API Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8fa762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Single API Processor\n",
    "Main function to process a single insurance company API.\n",
    "Handles pagination, chunking, and saves data incrementally to disk.\n",
    "\"\"\"\n",
    "\n",
    "async def process_single_api(\n",
    "    session: aiohttp.ClientSession,\n",
    "    url: str,\n",
    "    business_name: str,\n",
    "    api_index: int,\n",
    "    total_apis: int\n",
    ") -> Tuple[bool, int]:\n",
    "    \"\"\"\n",
    "    Process all pages for a single API endpoint\n",
    "    \n",
    "    Args:\n",
    "        session: aiohttp client session\n",
    "        url: API endpoint URL\n",
    "        business_name: Business identifier\n",
    "        api_index: Current API index for progress tracking\n",
    "        total_apis: Total number of APIs\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (success: bool, records_count: int)\n",
    "    \"\"\"\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    logger.info(f\"Processing API {api_index + 1}/{total_apis}: {business_name}\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch first page to get pagination info\n",
    "        init_data = await fetch_with_retry(session, url, business_name)\n",
    "        if not init_data:\n",
    "            logger.error(f\"Failed to fetch initial data for {business_name}\")\n",
    "            return False, 0\n",
    "        \n",
    "        # Get pagination information\n",
    "        if 'last_page' not in init_data:\n",
    "            logger.error(f\"No pagination info found for {business_name}\")\n",
    "            return False, 0\n",
    "        \n",
    "        try:\n",
    "            last_page = int(init_data['last_page'])\n",
    "            logger.info(f\"Found {last_page} pages for {business_name}\")\n",
    "        except (ValueError, TypeError):\n",
    "            logger.error(f\"Invalid last_page value for {business_name}\")\n",
    "            return False, 0\n",
    "        \n",
    "        # Process pages in batches\n",
    "        all_records = []\n",
    "        chunk_counter = 0\n",
    "        total_records = 0\n",
    "        \n",
    "        # Create progress bar\n",
    "        pbar = tqdm(total=last_page, desc=f\"{business_name}\", unit=\"page\")\n",
    "        \n",
    "        for page_num in range(1, last_page + 1):\n",
    "            # Construct page URL\n",
    "            base_url = init_data.get('next_page_url', url)\n",
    "            if '?' in base_url:\n",
    "                page_url = base_url.rsplit('?', 1)[0] + f'?page={page_num}'\n",
    "            else:\n",
    "                page_url = f\"{base_url}?page={page_num}\"\n",
    "            \n",
    "            # Fetch page data\n",
    "            page_data = await fetch_with_retry(session, page_url, business_name)\n",
    "            \n",
    "            if page_data and 'data' in page_data:\n",
    "                records = page_data['data']\n",
    "                all_records.extend(records)\n",
    "                total_records += len(records)\n",
    "                \n",
    "                # Save chunk when threshold reached\n",
    "                if len(all_records) >= CONFIG['CHUNK_SIZE']:\n",
    "                    await save_chunk_to_disk(all_records, business_name, chunk_counter)\n",
    "                    chunk_counter += 1\n",
    "                    all_records = []  # Clear memory\n",
    "            else:\n",
    "                logger.warning(f\"No data in page {page_num} for {business_name}\")\n",
    "            \n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Memory management: clear batch periodically\n",
    "            if page_num % CONFIG['BATCH_SIZE'] == 0:\n",
    "                logger.debug(f\"Batch checkpoint at page {page_num}\")\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Save remaining records\n",
    "        if all_records:\n",
    "            await save_chunk_to_disk(all_records, business_name, chunk_counter)\n",
    "        \n",
    "        logger.info(f\" Completed {business_name}: {total_records} total records\")\n",
    "        return True, total_records\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\" Failed processing {business_name}: {str(e)}\")\n",
    "        error_logger.error(f\"{business_name} | Fatal error | {str(e)}\")\n",
    "        return False, 0\n",
    "\n",
    "logger.info(\" Single API processor defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1932687",
   "metadata": {},
   "source": [
    "## Main Pipeline Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcdcb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Main Pipeline Orchestrator\n",
    "Coordinates the sequential processing of all APIs.\n",
    "Handles checkpointing, error recovery, and progress tracking.\n",
    "\"\"\"\n",
    "\n",
    "async def run_pipeline():\n",
    "    \"\"\"\n",
    "    Main pipeline execution function\n",
    "    Processes all APIs sequentially with checkpoint support\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    resume_index = checkpoint.get_resume_index()\n",
    "    \n",
    "    logger.info(f\"{'#'*60}\")\n",
    "    logger.info(f\"STARTING DATA COLLECTION PIPELINE\")\n",
    "    logger.info(f\"Total APIs: {len(INSURANCE_APIS)}\")\n",
    "    logger.info(f\"Resuming from index: {resume_index}\")\n",
    "    logger.info(f\"{'#'*60}\")\n",
    "    \n",
    "    # Create aiohttp session with custom connector settings\n",
    "    connector = aiohttp.TCPConnector(limit=CONFIG['MAX_CONCURRENT_REQUESTS'])\n",
    "    timeout = aiohttp.ClientTimeout(total=60)\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "        # Process APIs sequentially from resume point\n",
    "        for idx in range(resume_index, len(INSURANCE_APIS)):\n",
    "            url, business_name = INSURANCE_APIS[idx]\n",
    "            \n",
    "            logger.info(f\"\\n{'*'*60}\")\n",
    "            logger.info(f\"API {idx + 1}/{len(INSURANCE_APIS)}: {business_name}\")\n",
    "            logger.info(f\"{'*'*60}\")\n",
    "            \n",
    "            success, record_count = await process_single_api(\n",
    "                session,\n",
    "                url,\n",
    "                business_name,\n",
    "                idx,\n",
    "                len(INSURANCE_APIS)\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                checkpoint.mark_api_complete(business_name, record_count)\n",
    "                logger.info(f\" Checkpoint saved for {business_name}\")\n",
    "            else:\n",
    "                checkpoint.mark_api_failed(business_name, \"Processing failed\")\n",
    "                logger.warning(f\" Marked {business_name} as failed, continuing...\")\n",
    "            \n",
    "            # Brief pause between APIs to be gentle on resources\n",
    "            await asyncio.sleep(2)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    logger.info(f\"\\n{'#'*60}\")\n",
    "    logger.info(f\"PIPELINE COMPLETED\")\n",
    "    logger.info(f\"Total time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
    "    logger.info(f\"Total records: {checkpoint.state['total_records_processed']}\")\n",
    "    logger.info(f\"Completed APIs: {len(checkpoint.state['completed_apis'])}\")\n",
    "    logger.info(f\"Failed APIs: {len(checkpoint.state['failed_apis'])}\")\n",
    "    logger.info(f\"{'#'*60}\")\n",
    "\n",
    "logger.info(\" Pipeline orchestrator defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a91999",
   "metadata": {},
   "source": [
    "## Execute the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58834d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Execute Pipeline\n",
    "Run the main data collection pipeline.\n",
    "This cell executes the asynchronous pipeline.\n",
    "\"\"\"\n",
    "\n",
    "# Run the pipeline\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING DATA COLLECTION PIPELINE\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Execute async pipeline\n",
    "await run_pipeline()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA COLLECTION COMPLETED\")\n",
    "print(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da841a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "##This will continue from where it left off\n",
    "# Check current progress\n",
    "print(f\"Completed: {len(checkpoint.state['completed_apis'])} APIs\")\n",
    "print(f\"Failed: {len(checkpoint.state['failed_apis'])} APIs\")\n",
    "print(f\"Current position: {checkpoint.get_resume_index()}/{len(INSURANCE_APIS)}\")\n",
    "print(f\"Total records so far: {checkpoint.state['total_records_processed']:,}\")\n",
    "\n",
    "# Then resume\n",
    "await resume_pipeline()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d33b5",
   "metadata": {},
   "source": [
    "## Consolidate Data from Temporary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc27ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Consolidate Data\n",
    "Merge all temporary JSON chunks into a single CSV file.\n",
    "This step processes data in batches to minimize memory usage.\n",
    "\"\"\"\n",
    "\n",
    "def consolidate_all_data():\n",
    "    \"\"\"\n",
    "    Consolidate all temporary data files into final CSV\n",
    "    Uses chunked processing to minimize memory usage\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting data consolidation...\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONSOLIDATING DATA\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    final_output = Path(CONFIG['FINAL_OUTPUT'])\n",
    "    temp_dir = Path(CONFIG['TEMP_DIR'])\n",
    "    \n",
    "    # Get all unique business names from checkpoint\n",
    "    completed_apis = [api['name'] for api in checkpoint.state['completed_apis']]\n",
    "    \n",
    "    if not completed_apis:\n",
    "        logger.warning(\"No completed APIs found to consolidate\")\n",
    "        return\n",
    "    \n",
    "    # Write header\n",
    "    first_df = load_temp_files_for_business(completed_apis[0])\n",
    "    first_df.to_csv(final_output, index=False, mode='w')\n",
    "    logger.info(f\" Wrote header and data for {completed_apis[0]}\")\n",
    "    \n",
    "    # Append remaining businesses\n",
    "    for business_name in tqdm(completed_apis[1:], desc=\"Consolidating\"):\n",
    "        try:\n",
    "            df = load_temp_files_for_business(business_name)\n",
    "            if not df.empty:\n",
    "                df.to_csv(final_output, index=False, mode='a', header=False)\n",
    "                logger.info(f\" Appended {len(df)} records for {business_name}\")\n",
    "            else:\n",
    "                logger.warning(f\"No data found for {business_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to consolidate {business_name}: {e}\")\n",
    "    \n",
    "    # Get final file size and record count\n",
    "    final_df = pd.read_csv(final_output, nrows=5)  # Sample to verify\n",
    "    file_size_mb = final_output.stat().st_size / (1024 * 1024)\n",
    "    \n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"CONSOLIDATION COMPLETE\")\n",
    "    logger.info(f\"Output file: {final_output}\")\n",
    "    logger.info(f\"File size: {file_size_mb:.2f} MB\")\n",
    "    logger.info(f\"Total businesses: {len(completed_apis)}\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\n Final dataset saved to: {CONFIG['FINAL_OUTPUT']}\")\n",
    "    print(f\" File size: {file_size_mb:.2f} MB\")\n",
    "    print(f\" Total businesses processed: {len(completed_apis)}\")\n",
    "\n",
    "# Run consolidation\n",
    "consolidate_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede321c6",
   "metadata": {},
   "source": [
    "## Data Verification and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Verify Final Output\n",
    "Load and verify the consolidated dataset.\n",
    "Display summary statistics and sample records.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFYING FINAL OUTPUT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    # Load with chunking to avoid memory issues\n",
    "    chunk_iterator = pd.read_csv(CONFIG['FINAL_OUTPUT'], chunksize=10000)\n",
    "    \n",
    "    # Get basic stats without loading full dataset\n",
    "    total_rows = 0\n",
    "    business_counts = {}\n",
    "    \n",
    "    for chunk in chunk_iterator:\n",
    "        total_rows += len(chunk)\n",
    "        for business in chunk['Business_Name'].value_counts().items():\n",
    "            business_counts[business[0]] = business_counts.get(business[0], 0) + business[1]\n",
    "    \n",
    "    print(f\" Total records: {total_rows:,}\")\n",
    "    print(f\" Unique businesses: {len(business_counts)}\")\n",
    "    print(f\"\\nTop 10 businesses by review count:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    sorted_businesses = sorted(business_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for business, count in sorted_businesses:\n",
    "        print(f\"  {business:<40} {count:>10,} reviews\")\n",
    "    \n",
    "    # Display sample records (first 5 rows)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SAMPLE RECORDS (first 5 rows)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    sample_df = pd.read_csv(CONFIG['FINAL_OUTPUT'], nrows=5)\n",
    "    print(sample_df.to_string())\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DATASET INFORMATION\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Get column information without loading full dataset\n",
    "    full_df_sample = pd.read_csv(CONFIG['FINAL_OUTPUT'], nrows=1000)\n",
    "    print(\"\\nColumn Names and Types:\")\n",
    "    print(\"-\" * 60)\n",
    "    for col, dtype in full_df_sample.dtypes.items():\n",
    "        null_count = full_df_sample[col].isnull().sum()\n",
    "        print(f\"  {col:<30} {str(dtype):<15} ({null_count} nulls in sample)\")\n",
    "    \n",
    "    print(f\"\\n Data verification complete!\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\" Error: Output file not found at {CONFIG['FINAL_OUTPUT']}\")\n",
    "except Exception as e:\n",
    "    print(f\" Error during verification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a6ed1",
   "metadata": {},
   "source": [
    "## Pipeline Statistics and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2db7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Pipeline Execution Report\n",
    "Generate comprehensive report of the pipeline execution,\n",
    "including timing, success rates, and any failures.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE EXECUTION REPORT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Load checkpoint state\n",
    "report_data = checkpoint.state\n",
    "\n",
    "print(\"EXECUTION SUMMARY\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Start Time:              {report_data.get('start_time', 'N/A')}\")\n",
    "print(f\"Last Update:             {report_data.get('last_update', 'N/A')}\")\n",
    "print(f\"Total Records Processed: {report_data.get('total_records_processed', 0):,}\")\n",
    "print(f\"Total APIs:              {len(INSURANCE_APIS)}\")\n",
    "print(f\"Completed APIs:          {len(report_data.get('completed_apis', []))}\")\n",
    "print(f\"Failed APIs:             {len(report_data.get('failed_apis', []))}\")\n",
    "\n",
    "if report_data.get('completed_apis'):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"COMPLETED APIS\")\n",
    "    print(\"=\"*60)\n",
    "    for api in report_data['completed_apis']:\n",
    "        print(f\"\\n  {api['name']}\")\n",
    "        print(f\"    Records:   {api['records']:,}\")\n",
    "        print(f\"    Completed: {api['timestamp']}\")\n",
    "\n",
    "if report_data.get('failed_apis'):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FAILED APIS (Check error log for details)\")\n",
    "    print(\"=\"*60)\n",
    "    for api in report_data['failed_apis']:\n",
    "        print(f\"\\n  {api['name']}\")\n",
    "        print(f\"    Error:     {api['error']}\")\n",
    "        print(f\"    Timestamp: {api['timestamp']}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LOG FILES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Execution Log:  {CONFIG['LOG_FILE']}\")\n",
    "print(f\"  Error Log:      {CONFIG['ERROR_LOG']}\")\n",
    "print(f\"  Checkpoint:     {CONFIG['CHECKPOINT_FILE']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04cb694",
   "metadata": {},
   "source": [
    "## Cleanup Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1811129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Cleanup Utilities\n",
    "Optional functions to clean up temporary files after successful completion.\n",
    "ONLY run these after verifying your final CSV is correct!\n",
    "\"\"\"\n",
    "\n",
    "def cleanup_temp_files():\n",
    "    \"\"\"\n",
    "    Remove all temporary chunk files after successful consolidation\n",
    "    WARNING: Only run this after verifying final output!\n",
    "    \"\"\"\n",
    "    temp_dir = Path(CONFIG['TEMP_DIR'])\n",
    "    \n",
    "    response = input(\"\\n  This will DELETE all temporary files. Are you sure? (yes/no): \")\n",
    "    if response.lower() != 'yes':\n",
    "        print(\"Cleanup cancelled.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        json_files = list(temp_dir.glob(\"*.json\"))\n",
    "        print(f\"\\nFound {len(json_files)} temporary files to delete...\")\n",
    "        \n",
    "        deleted_count = 0\n",
    "        for file in json_files:\n",
    "            try:\n",
    "                file.unlink()\n",
    "                deleted_count += 1\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to delete {file}: {e}\")\n",
    "        \n",
    "        print(f\" Deleted {deleted_count} temporary files\")\n",
    "        \n",
    "        # Optionally remove temp directory if empty\n",
    "        if not any(temp_dir.iterdir()):\n",
    "            temp_dir.rmdir()\n",
    "            print(f\" Removed empty directory: {temp_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Cleanup failed: {e}\")\n",
    "\n",
    "def reset_pipeline():\n",
    "    \"\"\"\n",
    "    Reset the entire pipeline (checkpoint and temp files)\n",
    "    WARNING: This will force a complete restart!\n",
    "    \"\"\"\n",
    "    response = input(\"\\n  This will RESET the entire pipeline. Are you sure? (yes/no): \")\n",
    "    if response.lower() != 'yes':\n",
    "        print(\"Reset cancelled.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Reset checkpoint\n",
    "        checkpoint.reset()\n",
    "        print(\" Checkpoint reset\")\n",
    "        \n",
    "        # Clean temp files\n",
    "        temp_dir = Path(CONFIG['TEMP_DIR'])\n",
    "        for file in temp_dir.glob(\"*.json\"):\n",
    "            file.unlink()\n",
    "        print(\" Temporary files deleted\")\n",
    "        \n",
    "        print(\"\\n Pipeline reset complete. You can now run from the beginning.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Reset failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLEANUP UTILITIES LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"  cleanup_temp_files()  - Remove temporary JSON chunks\")\n",
    "print(\"  reset_pipeline()      - Reset checkpoint and start over\")\n",
    "print(\"\\n  Only use these after verifying your final CSV!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc432ad",
   "metadata": {},
   "source": [
    "## Resume Pipeline (if interrupted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Resume Pipeline\n",
    "If the pipeline was interrupted, use this cell to resume from where it left off.\n",
    "The checkpoint system will automatically start from the last completed API.\n",
    "\"\"\"\n",
    "\n",
    "async def resume_pipeline():\n",
    "    \"\"\"\n",
    "    Resume pipeline execution from last checkpoint\n",
    "    \"\"\"\n",
    "    resume_index = checkpoint.get_resume_index()\n",
    "    \n",
    "    if resume_index >= len(INSURANCE_APIS):\n",
    "        print(\" All APIs have been processed!\")\n",
    "        print(\"  Run the consolidation cell if you haven't already.\")\n",
    "        return\n",
    "    \n",
    "    remaining = len(INSURANCE_APIS) - resume_index\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESUMING PIPELINE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Completed:  {resume_index} APIs\")\n",
    "    print(f\"  Remaining:  {remaining} APIs\")\n",
    "    print(f\"  Starting from: {INSURANCE_APIS[resume_index][1]}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    response = input(\"Continue? (yes/no): \")\n",
    "    if response.lower() != 'yes':\n",
    "        print(\"Resume cancelled.\")\n",
    "        return\n",
    "    \n",
    "    # Run the pipeline (it will automatically resume)\n",
    "    await run_pipeline()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESUME FUNCTIONALITY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTo resume an interrupted pipeline, run:\")\n",
    "print(\"  await resume_pipeline()\")\n",
    "print(f\"\\nCurrent progress: {checkpoint.get_resume_index()}/{len(INSURANCE_APIS)} APIs completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd5c56",
   "metadata": {},
   "source": [
    "## Memory Usage Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861be021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Memory Usage Monitor\n",
    "Track memory usage during execution to ensure we stay within limits.\n",
    "Useful for debugging and optimization.\n",
    "\"\"\"\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage statistics\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    \n",
    "    # Get system memory\n",
    "    system_memory = psutil.virtual_memory()\n",
    "    \n",
    "    return {\n",
    "        'process_mb': memory_info.rss / (1024 * 1024),\n",
    "        'system_total_mb': system_memory.total / (1024 * 1024),\n",
    "        'system_used_mb': system_memory.used / (1024 * 1024),\n",
    "        'system_percent': system_memory.percent\n",
    "    }\n",
    "\n",
    "def print_memory_status():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    try:\n",
    "        mem = get_memory_usage()\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"MEMORY USAGE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  Process Memory:      {mem['process_mb']:.2f} MB\")\n",
    "        print(f\"  System Memory Used:  {mem['system_used_mb']:.2f} MB / {mem['system_total_mb']:.2f} MB\")\n",
    "        print(f\"  System Memory:       {mem['system_percent']:.1f}% used\")\n",
    "        print(\"=\"*60)\n",
    "    except ImportError:\n",
    "        print(\"  psutil not installed. Run: pip install psutil\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error getting memory info: {e}\")\n",
    "\n",
    "# Show current memory usage\n",
    "print_memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf37508",
   "metadata": {},
   "source": [
    "## Advanced Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c430f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Quick Data Analytics\n",
    "Perform quick analytics on the collected data without loading everything into memory.\n",
    "\"\"\"\n",
    "\n",
    "def analyze_dataset():\n",
    "    \"\"\"\n",
    "    Perform quick analytics on the final dataset\n",
    "    Uses chunked processing for memory efficiency\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATASET ANALYTICS\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    if not Path(CONFIG['FINAL_OUTPUT']).exists():\n",
    "        print(f\" Dataset not found at {CONFIG['FINAL_OUTPUT']}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Initialize counters\n",
    "        total_records = 0\n",
    "        business_stats = {}\n",
    "        \n",
    "        # Process in chunks\n",
    "        print(\"Processing data in chunks...\")\n",
    "        for chunk in pd.read_csv(CONFIG['FINAL_OUTPUT'], chunksize=5000):\n",
    "            total_records += len(chunk)\n",
    "            \n",
    "            # Count by business\n",
    "            for business, count in chunk['Business_Name'].value_counts().items():\n",
    "                business_stats[business] = business_stats.get(business, 0) + count\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"OVERVIEW\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  Total Reviews:       {total_records:,}\")\n",
    "        print(f\"  Unique Businesses:   {len(business_stats)}\")\n",
    "        print(f\"  Average per Business: {total_records / len(business_stats):.0f}\")\n",
    "        \n",
    "        # Top and bottom performers\n",
    "        sorted_businesses = sorted(business_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"TOP 10 BUSINESSES (Most Reviews)\")\n",
    "        print(\"=\"*60)\n",
    "        for i, (business, count) in enumerate(sorted_businesses[:10], 1):\n",
    "            pct = (count / total_records) * 100\n",
    "            print(f\"  {i:2d}. {business:<35} {count:>8,} ({pct:>5.2f}%)\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"BOTTOM 10 BUSINESSES (Least Reviews)\")\n",
    "        print(\"=\"*60)\n",
    "        for i, (business, count) in enumerate(sorted_businesses[-10:], 1):\n",
    "            pct = (count / total_records) * 100\n",
    "            print(f\"  {i:2d}. {business:<35} {count:>8,} ({pct:>5.2f}%)\")\n",
    "        \n",
    "        # File size\n",
    "        file_size_mb = Path(CONFIG['FINAL_OUTPUT']).stat().st_size / (1024 * 1024)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"FILE INFORMATION\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  File Size:     {file_size_mb:.2f} MB\")\n",
    "        print(f\"  Average KB per record: {(file_size_mb * 1024) / total_records:.2f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error during analysis: {e}\")\n",
    "        logger.error(f\"Analytics failed: {e}\")\n",
    "\n",
    "print(\"\\nAnalytics function loaded. Run with:\")\n",
    "print(\"  analyze_dataset()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422cdb46",
   "metadata": {},
   "source": [
    "## Final Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81552456",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Pipeline Complete! \n",
    "\n",
    "data collection pipeline has been successfully set up and executed.\n",
    "\n",
    "### What Was Accomplished:\n",
    "-  Collected reviews from multiple insurance company APIs\n",
    "-  Implemented fault-tolerant retry logic with exponential backoff\n",
    "-  Used asynchronous requests for efficient network I/O\n",
    "-  Saved data incrementally to minimize memory usage\n",
    "-  Created checkpoint system for resumable execution\n",
    "-  Consolidated all data into a single CSV file\n",
    "\n",
    "### Output Files:\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# List all output files\n",
    "output_files = {\n",
    "    'Final Dataset': CONFIG['FINAL_OUTPUT'],\n",
    "    'Execution Log': CONFIG['LOG_FILE'],\n",
    "    'Error Log': CONFIG['ERROR_LOG'],\n",
    "    'Checkpoint': CONFIG['CHECKPOINT_FILE'],\n",
    "    'Temp Directory': CONFIG['TEMP_DIR']\n",
    "}\n",
    "\n",
    "print(\"OUTPUT FILES:\")\n",
    "print(\"-\" * 60)\n",
    "for name, path in output_files.items():\n",
    "    file_path = Path(path)\n",
    "    if file_path.exists():\n",
    "        if file_path.is_file():\n",
    "            size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"   {name:<20} {path:<35} ({size_mb:.2f} MB)\")\n",
    "        else:\n",
    "            file_count = len(list(file_path.glob('*')))\n",
    "            print(f\"   {name:<20} {path:<35} ({file_count} files)\")\n",
    "    else:\n",
    "        print(f\"   {name:<20} {path:<35} (not found)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "1. Verify  data:\n",
    "   - Check the final CSV file\n",
    "   - Review the execution and error logs\n",
    "   - Run analytics: analyze_dataset()\n",
    "\n",
    "2. Clean up (optional):\n",
    "   - Remove temp files: cleanup_temp_files()\n",
    "   - Reset for fresh run: reset_pipeline()\n",
    "\n",
    "3. Export to other formats (optional):\n",
    "   - export_to_format('json')\n",
    "   - export_to_format('parquet')\n",
    "\n",
    "4. If interrupted:\n",
    "   - Resume from checkpoint: await resume_pipeline()\n",
    "\n",
    "5. Memory monitoring:\n",
    "   - Check usage: print_memory_status()\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Thank you for using the Data Collection Pipeline!\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
