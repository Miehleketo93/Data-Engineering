{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb94a147",
   "metadata": {},
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "04ab9364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ready to import libraries\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Environment Setup\n",
    "Install all required libraries for the data collection pipeline.\n",
    "This cell only needs to be run once per environment.\n",
    "\"\"\"\n",
    "#!pip install -q aiohttp aiofiles pandas tqdm psutil requests beautifulsoup4 scrapy spacy nltk transformers datasets\n",
    "\n",
    "print(\" Ready to import libraries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac6314",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "474e7fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Import Required Libraries\n",
    "Import all necessary libraries for asynchronous API calls, data processing,\n",
    "and checkpoint management.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from tqdm.asyncio import tqdm\n",
    "import sys\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "print(\" All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff7235",
   "metadata": {},
   "source": [
    "## Configuration and Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "190dedf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:56:42,797 - INFO -  Configuration and logging initialized\n",
      " Configuration loaded - Max concurrent requests: 3\n",
      " Checkpoint file: pipeline_checkpoint.json\n",
      " Temporary data directory: temp_data\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Configuration Settings\n",
    "Configure all pipeline parameters including rate limits, retry settings,\n",
    "and file paths. Adjust these based on your needs and API limits.\n",
    "\"\"\"\n",
    "\n",
    "# ============= CONFIGURATION =============\n",
    "CONFIG = {\n",
    "    # Rate limiting (requests per second)\n",
    "    'MAX_CONCURRENT_REQUESTS': 3,  # Reduced for 8GB RAM\n",
    "    'REQUEST_DELAY': 0.5,  # Delay between requests in seconds\n",
    "    \n",
    "    # Retry settings\n",
    "    'MAX_RETRIES': 5,\n",
    "    'INITIAL_BACKOFF': 2,  # Initial backoff in seconds\n",
    "    'MAX_BACKOFF': 60,  # Maximum backoff in seconds\n",
    "    \n",
    "    # Memory management\n",
    "    'CHUNK_SIZE': 100,  # Save to disk every N records\n",
    "    'BATCH_SIZE': 50,  # Process N pages before clearing memory\n",
    "    \n",
    "    # File paths\n",
    "    'CHECKPOINT_FILE': 'pipeline_checkpoint.json',\n",
    "    'LOG_FILE': 'pipeline_execution.log',\n",
    "    'ERROR_LOG': 'pipeline_errors.log',\n",
    "    'TEMP_DIR': 'temp_data',\n",
    "    'FINAL_OUTPUT': 'customer_utilities_reviews_final.csv',\n",
    "}\n",
    "\n",
    "# Create necessary directories\n",
    "Path(CONFIG['TEMP_DIR']).mkdir(exist_ok=True)\n",
    "\n",
    "# ============= LOGGING SETUP =============\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(CONFIG['LOG_FILE']),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Separate error logger\n",
    "error_logger = logging.getLogger('errors')\n",
    "error_handler = logging.FileHandler(CONFIG['ERROR_LOG'])\n",
    "error_handler.setLevel(logging.ERROR)\n",
    "error_logger.addHandler(error_handler)\n",
    "\n",
    "logger.info(\" Configuration and logging initialized\")\n",
    "print(f\" Configuration loaded - Max concurrent requests: {CONFIG['MAX_CONCURRENT_REQUESTS']}\")\n",
    "print(f\" Checkpoint file: {CONFIG['CHECKPOINT_FILE']}\")\n",
    "print(f\" Temporary data directory: {CONFIG['TEMP_DIR']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c695fc",
   "metadata": {},
   "source": [
    "## API Endpoints Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8009e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Hellopeter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m## API Endpoints Configuration\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mDefine all insurance company APIs to be scraped. Each entry contains\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mthe API URL and a business identifier.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mHellopeter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mData_Pipelines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m INSURANCE_APIS  \u001b[38;5;66;03m# API list imported from config.py\u001b[39;00m\n\u001b[0;32m      9\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Configured \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(INSURANCE_APIS)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m insurance APIs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Total APIs to process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(INSURANCE_APIS)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Hellopeter'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## API Endpoints Configuration\n",
    "Define all insurance company APIs to be scraped. Each entry contains\n",
    "the API URL and a business identifier.\n",
    "\"\"\"\n",
    "from config import INSURANCE_APIS  # API list imported from config.py\n",
    "\n",
    "\n",
    "logger.info(f\" Configured {len(INSURANCE_APIS)} insurance APIs\")\n",
    "print(f\" Total APIs to process: {len(INSURANCE_APIS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f1ea91",
   "metadata": {},
   "source": [
    "## Checkpoint Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e831500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:34:35,550 - INFO -  Checkpoint manager initialized\n",
      " Will resume from API index: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Checkpoint Management\n",
    "Functions to save and load pipeline progress. This allows the pipeline\n",
    "to resume from where it left off if interrupted.\n",
    "\"\"\"\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"Manages pipeline checkpoint state for resume capability\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_file: str):\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "        self.state = self._load_checkpoint()\n",
    "    \n",
    "    def _load_checkpoint(self) -> Dict:\n",
    "        \"\"\"Load checkpoint from file or create new\"\"\"\n",
    "        if Path(self.checkpoint_file).exists():\n",
    "            try:\n",
    "                with open(self.checkpoint_file, 'r') as f:\n",
    "                    state = json.load(f)\n",
    "                logger.info(f\" Loaded checkpoint from {self.checkpoint_file}\")\n",
    "                return state\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load checkpoint: {e}. Starting fresh.\")\n",
    "                return self._create_empty_checkpoint()\n",
    "        return self._create_empty_checkpoint()\n",
    "    \n",
    "    def _create_empty_checkpoint(self) -> Dict:\n",
    "        \"\"\"Create empty checkpoint structure\"\"\"\n",
    "        return {\n",
    "            'current_api_index': 0,\n",
    "            'completed_apis': [],\n",
    "            'failed_apis': [],\n",
    "            'last_update': None,\n",
    "            'total_records_processed': 0,\n",
    "            'start_time': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        \"\"\"Save current state to file\"\"\"\n",
    "        self.state['last_update'] = datetime.now().isoformat()\n",
    "        try:\n",
    "            with open(self.checkpoint_file, 'w') as f:\n",
    "                json.dump(self.state, f, indent=2)\n",
    "            logger.debug(f\"Checkpoint saved: API {self.state['current_api_index']}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save checkpoint: {e}\")\n",
    "    \n",
    "    def mark_api_complete(self, api_name: str, records_count: int):\n",
    "        \"\"\"Mark an API as completed\"\"\"\n",
    "        self.state['completed_apis'].append({\n",
    "            'name': api_name,\n",
    "            'records': records_count,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        self.state['current_api_index'] += 1\n",
    "        self.state['total_records_processed'] += records_count\n",
    "        self.save_checkpoint()\n",
    "    \n",
    "    def mark_api_failed(self, api_name: str, error: str):\n",
    "        \"\"\"Mark an API as failed\"\"\"\n",
    "        self.state['failed_apis'].append({\n",
    "            'name': api_name,\n",
    "            'error': str(error),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        self.state['current_api_index'] += 1\n",
    "        self.save_checkpoint()\n",
    "    \n",
    "    def get_resume_index(self) -> int:\n",
    "        \"\"\"Get the index to resume from\"\"\"\n",
    "        return self.state['current_api_index']\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset checkpoint to start fresh\"\"\"\n",
    "        self.state = self._create_empty_checkpoint()\n",
    "        self.save_checkpoint()\n",
    "        logger.info(\"Checkpoint reset\")\n",
    "\n",
    "# Initialize checkpoint manager\n",
    "checkpoint = CheckpointManager(CONFIG['CHECKPOINT_FILE'])\n",
    "logger.info(\" Checkpoint manager initialized\")\n",
    "print(f\" Will resume from API index: {checkpoint.get_resume_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f1375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "API DIAGNOSTICS - Run this FIRST!\n",
      "============================================================\n",
      "\n",
      "This will test if the HelloPeter API is accessible.\n",
      "Run the cell below to test:\n",
      "\n",
      "  await test_single_api(\n",
      "  )\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## API Diagnostic Tool\n",
    "Test a single API endpoint to verify it's working correctly.\n",
    "Run this BEFORE the full pipeline to diagnose issues!\n",
    "\"\"\"\n",
    "\n",
    "async def test_single_api(business_url: str, business_name: str):\n",
    "    \"\"\"\n",
    "    Test a single API endpoint with detailed diagnostics\n",
    "    \n",
    "    Args:\n",
    "        business_url: API endpoint to test\n",
    "        business_name: Business identifier\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TESTING API: {business_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"URL: {business_url}\\n\")\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "    }\n",
    "    \n",
    "    connector = aiohttp.TCPConnector(limit=1)\n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        try:\n",
    "            print(\" Making request...\")\n",
    "            async with session.get(business_url, headers=headers, timeout=aiohttp.ClientTimeout(total=10)) as response:\n",
    "                status = response.status\n",
    "                print(f\" Status Code: {status}\")\n",
    "                print(f\" Headers: {dict(response.headers)}\\n\")\n",
    "                \n",
    "                if status == 200:\n",
    "                    data = await response.json()\n",
    "                    print(\" SUCCESS! API returned data\")\n",
    "                    print(f\"\\n Response Structure:\")\n",
    "                    print(f\"   Keys: {list(data.keys())}\")\n",
    "                    \n",
    "                    if 'data' in data:\n",
    "                        print(f\"   Data records: {len(data.get('data', []))}\")\n",
    "                        if data['data']:\n",
    "                            print(f\"   Sample record keys: {list(data['data'][0].keys())}\")\n",
    "                    \n",
    "                    if 'last_page' in data:\n",
    "                        print(f\"   Total pages: {data.get('last_page')}\")\n",
    "                    \n",
    "                    return True, data\n",
    "                \n",
    "                elif status == 202:\n",
    "                    text = await response.text()\n",
    "                    print(f\"  HTTP 202 (Accepted) - API is queuing requests\")\n",
    "                    print(f\"   This usually means authentication required or rate limiting\")\n",
    "                    print(f\"   Response: {text[:200]}\")\n",
    "                    return False, None\n",
    "                \n",
    "                else:\n",
    "                    text = await response.text()\n",
    "                    print(f\" HTTP {status}\")\n",
    "                    print(f\"   Response: {text[:500]}\")\n",
    "                    return False, None\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\" ERROR: {str(e)}\")\n",
    "            return False, None\n",
    "\n",
    "# Test with one API\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"API DIAGNOSTICS - Run this FIRST!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nThis will test if the HelloPeter API is accessible.\")\n",
    "print(\"Run the cell below to test:\\n\")\n",
    "print(\"  await test_single_api(\")\n",
    "print(\"  )\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea92240b",
   "metadata": {},
   "source": [
    "## Rate Limiter Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc761f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:34:35,622 - INFO -  Rate limiter initialized\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Rate Limiter\n",
    "Implements token bucket algorithm for rate limiting API requests.\n",
    "Prevents overwhelming the API and ensures compliance with rate limits.\n",
    "\"\"\"\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Token bucket rate limiter for API requests\"\"\"\n",
    "    \n",
    "    def __init__(self, max_requests: int, delay: float):\n",
    "        self.max_requests = max_requests\n",
    "        self.delay = delay\n",
    "        self.semaphore = asyncio.Semaphore(max_requests)\n",
    "        self.last_request_time = 0\n",
    "    \n",
    "    async def acquire(self):\n",
    "        \"\"\"Acquire permission to make a request\"\"\"\n",
    "        async with self.semaphore:\n",
    "            # Ensure minimum delay between requests\n",
    "            current_time = time.time()\n",
    "            time_since_last = current_time - self.last_request_time\n",
    "            if time_since_last < self.delay:\n",
    "                await asyncio.sleep(self.delay - time_since_last)\n",
    "            self.last_request_time = time.time()\n",
    "\n",
    "rate_limiter = RateLimiter(\n",
    "    CONFIG['MAX_CONCURRENT_REQUESTS'],\n",
    "    CONFIG['REQUEST_DELAY']\n",
    ")\n",
    "logger.info(\" Rate limiter initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac4cb7f",
   "metadata": {},
   "source": [
    "## Async HTTP Fetcher with Retry Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce718f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:34:35,667 - INFO - ✓ Async fetcher with retry logic defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Async HTTP Fetcher\n",
    "Asynchronous HTTP request handler with exponential backoff retry logic.\n",
    "Handles network errors, timeouts, and rate limiting gracefully.\n",
    "\"\"\"\n",
    "\n",
    "async def fetch_with_retry(\n",
    "    session: aiohttp.ClientSession,\n",
    "    url: str,\n",
    "    business_name: str,\n",
    "    attempt: int = 0\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch data from API with exponential backoff retry logic\n",
    "    \n",
    "    Args:\n",
    "        session: aiohttp client session\n",
    "        url: API endpoint URL\n",
    "        business_name: Business identifier for logging\n",
    "        attempt: Current retry attempt number\n",
    "    \n",
    "    Returns:\n",
    "        JSON response as dictionary or None if failed\n",
    "    \"\"\"\n",
    "    if attempt >= CONFIG['MAX_RETRIES']:\n",
    "        error_msg = f\"Max retries ({CONFIG['MAX_RETRIES']}) reached for {business_name}\"\n",
    "        logger.error(error_msg)\n",
    "        error_logger.error(f\"{business_name} | {url} | {error_msg}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Apply rate limiting\n",
    "        await rate_limiter.acquire()\n",
    "        \n",
    "        # Add headers to mimic browser request\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "            'Accept': 'application/json',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "        }\n",
    "        \n",
    "        # Make the request with timeout\n",
    "        async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=30)) as response:\n",
    "            # HelloPeter API returns 202 with valid data - accept both 200 and 202\n",
    "            if response.status == 200 or response.status == 202:\n",
    "                try:\n",
    "                    data = await response.json()\n",
    "                    # Verify we got actual data, not just an acknowledgment\n",
    "                    if data and isinstance(data, dict):\n",
    "                        return data\n",
    "                    else:\n",
    "                        logger.warning(f\"Empty or invalid response for {business_name}\")\n",
    "                        return None\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to parse JSON for {business_name}: {e}\")\n",
    "                    return None\n",
    "            elif response.status == 429:  # Too many requests\n",
    "                backoff = min(CONFIG['MAX_BACKOFF'], CONFIG['INITIAL_BACKOFF'] * (2 ** attempt))\n",
    "                logger.warning(f\"Rate limited for {business_name}. Retrying in {backoff}s (attempt {attempt + 1}/{CONFIG['MAX_RETRIES']})\")\n",
    "                await asyncio.sleep(backoff)\n",
    "                return await fetch_with_retry(session, url, business_name, attempt + 1)\n",
    "            elif response.status == 404:\n",
    "                logger.error(f\"API endpoint not found (404) for {business_name}: {url}\")\n",
    "                return None\n",
    "            else:\n",
    "                logger.warning(f\"HTTP {response.status} for {business_name}\")\n",
    "                return None\n",
    "    \n",
    "    except asyncio.TimeoutError:\n",
    "        backoff = min(CONFIG['MAX_BACKOFF'], CONFIG['INITIAL_BACKOFF'] * (2 ** attempt))\n",
    "        logger.warning(f\"Timeout for {business_name}. Retrying in {backoff}s (attempt {attempt + 1}/{CONFIG['MAX_RETRIES']})\")\n",
    "        await asyncio.sleep(backoff)\n",
    "        return await fetch_with_retry(session, url, business_name, attempt + 1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        backoff = min(CONFIG['MAX_BACKOFF'], CONFIG['INITIAL_BACKOFF'] * (2 ** attempt))\n",
    "        logger.warning(f\"Error for {business_name}: {str(e)}. Retrying in {backoff}s (attempt {attempt + 1}/{CONFIG['MAX_RETRIES']})\")\n",
    "        error_logger.error(f\"{business_name} | {url} | {str(e)}\")\n",
    "        await asyncio.sleep(backoff)\n",
    "        return await fetch_with_retry(session, url, business_name, attempt + 1)\n",
    "\n",
    "logger.info(\"✓ Async fetcher with retry logic defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc069d",
   "metadata": {},
   "source": [
    "## Data Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee5b80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:34:35,702 - INFO -  Data storage functions defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Data Storage Functions\n",
    "Functions to save data to disk in chunks to minimize memory usage.\n",
    "Uses JSON format for temporary storage and CSV for final output.\n",
    "\"\"\"\n",
    "\n",
    "async def save_chunk_to_disk(data: List[Dict], business_name: str, chunk_id: int):\n",
    "    \"\"\"\n",
    "    Save a chunk of data to temporary JSON file\n",
    "    \n",
    "    Args:\n",
    "        data: List of review records\n",
    "        business_name: Business identifier\n",
    "        chunk_id: Unique chunk identifier\n",
    "    \"\"\"\n",
    "    filename = Path(CONFIG['TEMP_DIR']) / f\"{business_name}_chunk_{chunk_id}.json\"\n",
    "    try:\n",
    "        async with aiofiles.open(filename, 'w') as f:\n",
    "            await f.write(json.dumps(data))\n",
    "        logger.debug(f\"Saved chunk {chunk_id} for {business_name} ({len(data)} records)\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save chunk {chunk_id} for {business_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_temp_files_for_business(business_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all temporary chunks for a business and return as DataFrame\n",
    "    \n",
    "    Args:\n",
    "        business_name: Business identifier\n",
    "    \n",
    "    Returns:\n",
    "        Combined DataFrame of all chunks\n",
    "    \"\"\"\n",
    "    temp_dir = Path(CONFIG['TEMP_DIR'])\n",
    "    pattern = f\"{business_name}_chunk_*.json\"\n",
    "    chunk_files = sorted(temp_dir.glob(pattern))\n",
    "    \n",
    "    if not chunk_files:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    all_data = []\n",
    "    for chunk_file in chunk_files:\n",
    "        try:\n",
    "            with open(chunk_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                all_data.extend(data)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load {chunk_file}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    df['Business_Name'] = business_name\n",
    "    return df\n",
    "\n",
    "logger.info(\" Data storage functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1fd2e5",
   "metadata": {},
   "source": [
    "## Single API Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8fa762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:34:35,752 - INFO -  Single API processor defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Single API Processor\n",
    "Main function to process a single insurance company API.\n",
    "Handles pagination, chunking, and saves data incrementally to disk.\n",
    "\"\"\"\n",
    "\n",
    "async def process_single_api(\n",
    "    session: aiohttp.ClientSession,\n",
    "    url: str,\n",
    "    business_name: str,\n",
    "    api_index: int,\n",
    "    total_apis: int\n",
    ") -> Tuple[bool, int]:\n",
    "    \"\"\"\n",
    "    Process all pages for a single API endpoint\n",
    "    \n",
    "    Args:\n",
    "        session: aiohttp client session\n",
    "        url: API endpoint URL\n",
    "        business_name: Business identifier\n",
    "        api_index: Current API index for progress tracking\n",
    "        total_apis: Total number of APIs\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (success: bool, records_count: int)\n",
    "    \"\"\"\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    logger.info(f\"Processing API {api_index + 1}/{total_apis}: {business_name}\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch first page to get pagination info\n",
    "        init_data = await fetch_with_retry(session, url, business_name)\n",
    "        if not init_data:\n",
    "            logger.error(f\"Failed to fetch initial data for {business_name}\")\n",
    "            return False, 0\n",
    "        \n",
    "        # Get pagination information\n",
    "        if 'last_page' not in init_data:\n",
    "            logger.error(f\"No pagination info found for {business_name}\")\n",
    "            return False, 0\n",
    "        \n",
    "        try:\n",
    "            last_page = int(init_data['last_page'])\n",
    "            logger.info(f\"Found {last_page} pages for {business_name}\")\n",
    "        except (ValueError, TypeError):\n",
    "            logger.error(f\"Invalid last_page value for {business_name}\")\n",
    "            return False, 0\n",
    "        \n",
    "        # Process pages in batches\n",
    "        all_records = []\n",
    "        chunk_counter = 0\n",
    "        total_records = 0\n",
    "        \n",
    "        # Create progress bar\n",
    "        pbar = tqdm(total=last_page, desc=f\"{business_name}\", unit=\"page\")\n",
    "        \n",
    "        for page_num in range(1, last_page + 1):\n",
    "            # Construct page URL\n",
    "            base_url = init_data.get('next_page_url', url)\n",
    "            if '?' in base_url:\n",
    "                page_url = base_url.rsplit('?', 1)[0] + f'?page={page_num}'\n",
    "            else:\n",
    "                page_url = f\"{base_url}?page={page_num}\"\n",
    "            \n",
    "            # Fetch page data\n",
    "            page_data = await fetch_with_retry(session, page_url, business_name)\n",
    "            \n",
    "            if page_data and 'data' in page_data:\n",
    "                records = page_data['data']\n",
    "                all_records.extend(records)\n",
    "                total_records += len(records)\n",
    "                \n",
    "                # Save chunk when threshold reached\n",
    "                if len(all_records) >= CONFIG['CHUNK_SIZE']:\n",
    "                    await save_chunk_to_disk(all_records, business_name, chunk_counter)\n",
    "                    chunk_counter += 1\n",
    "                    all_records = []  # Clear memory\n",
    "            else:\n",
    "                logger.warning(f\"No data in page {page_num} for {business_name}\")\n",
    "            \n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Memory management: clear batch periodically\n",
    "            if page_num % CONFIG['BATCH_SIZE'] == 0:\n",
    "                logger.debug(f\"Batch checkpoint at page {page_num}\")\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Save remaining records\n",
    "        if all_records:\n",
    "            await save_chunk_to_disk(all_records, business_name, chunk_counter)\n",
    "        \n",
    "        logger.info(f\" Completed {business_name}: {total_records} total records\")\n",
    "        return True, total_records\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\" Failed processing {business_name}: {str(e)}\")\n",
    "        error_logger.error(f\"{business_name} | Fatal error | {str(e)}\")\n",
    "        return False, 0\n",
    "\n",
    "logger.info(\" Single API processor defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1932687",
   "metadata": {},
   "source": [
    "## Main Pipeline Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcdcb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:34:35,821 - INFO -  Pipeline orchestrator defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Main Pipeline Orchestrator\n",
    "Coordinates the sequential processing of all APIs.\n",
    "Handles checkpointing, error recovery, and progress tracking.\n",
    "\"\"\"\n",
    "\n",
    "async def run_pipeline():\n",
    "    \"\"\"\n",
    "    Main pipeline execution function\n",
    "    Processes all APIs sequentially with checkpoint support\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    resume_index = checkpoint.get_resume_index()\n",
    "    \n",
    "    logger.info(f\"{'#'*60}\")\n",
    "    logger.info(f\"STARTING DATA COLLECTION PIPELINE\")\n",
    "    logger.info(f\"Total APIs: {len(INSURANCE_APIS)}\")\n",
    "    logger.info(f\"Resuming from index: {resume_index}\")\n",
    "    logger.info(f\"{'#'*60}\")\n",
    "    \n",
    "    # Create aiohttp session with custom connector settings\n",
    "    connector = aiohttp.TCPConnector(limit=CONFIG['MAX_CONCURRENT_REQUESTS'])\n",
    "    timeout = aiohttp.ClientTimeout(total=60)\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "        # Process APIs sequentially from resume point\n",
    "        for idx in range(resume_index, len(INSURANCE_APIS)):\n",
    "            url, business_name = INSURANCE_APIS[idx]\n",
    "            \n",
    "            logger.info(f\"\\n{'*'*60}\")\n",
    "            logger.info(f\"API {idx + 1}/{len(INSURANCE_APIS)}: {business_name}\")\n",
    "            logger.info(f\"{'*'*60}\")\n",
    "            \n",
    "            success, record_count = await process_single_api(\n",
    "                session,\n",
    "                url,\n",
    "                business_name,\n",
    "                idx,\n",
    "                len(INSURANCE_APIS)\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                checkpoint.mark_api_complete(business_name, record_count)\n",
    "                logger.info(f\" Checkpoint saved for {business_name}\")\n",
    "            else:\n",
    "                checkpoint.mark_api_failed(business_name, \"Processing failed\")\n",
    "                logger.warning(f\" Marked {business_name} as failed, continuing...\")\n",
    "            \n",
    "            # Brief pause between APIs to be gentle on resources\n",
    "            await asyncio.sleep(2)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    logger.info(f\"\\n{'#'*60}\")\n",
    "    logger.info(f\"PIPELINE COMPLETED\")\n",
    "    logger.info(f\"Total time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
    "    logger.info(f\"Total records: {checkpoint.state['total_records_processed']}\")\n",
    "    logger.info(f\"Completed APIs: {len(checkpoint.state['completed_apis'])}\")\n",
    "    logger.info(f\"Failed APIs: {len(checkpoint.state['failed_apis'])}\")\n",
    "    logger.info(f\"{'#'*60}\")\n",
    "\n",
    "logger.info(\" Pipeline orchestrator defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a91999",
   "metadata": {},
   "source": [
    "## Execute the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58834d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING DATA COLLECTION PIPELINE\n",
      "============================================================\n",
      "\n",
      "2025-11-02 13:34:35,860 - INFO - ############################################################\n",
      "2025-11-02 13:34:35,862 - INFO - STARTING DATA COLLECTION PIPELINE\n",
      "2025-11-02 13:34:35,865 - INFO - Total APIs: 17\n",
      "2025-11-02 13:34:35,867 - INFO - Resuming from index: 0\n",
      "2025-11-02 13:34:35,873 - INFO - ############################################################\n",
      "2025-11-02 13:34:35,878 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:34:35,878 - INFO - API 1/17: pentravel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:34:35,884 - INFO - ************************************************************\n",
      "2025-11-02 13:34:35,899 - INFO - ============================================================\n",
      "2025-11-02 13:34:35,903 - INFO - Processing API 1/17: pentravel\n",
      "2025-11-02 13:34:35,905 - INFO - ============================================================\n",
      "2025-11-02 13:34:36,717 - INFO - Found 144 pages for pentravel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pentravel: 100%|██████████| 144/144 [02:18<00:00,  1.04page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:36:54,875 - INFO -  Completed pentravel: 1579 total records\n",
      "2025-11-02 13:36:54,889 - INFO -  Checkpoint saved for pentravel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:36:56,909 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:36:56,916 - INFO - API 2/17: beekman-holidays\n",
      "2025-11-02 13:36:56,923 - INFO - ************************************************************\n",
      "2025-11-02 13:36:56,924 - INFO - ============================================================\n",
      "2025-11-02 13:36:56,931 - INFO - Processing API 2/17: beekman-holidays\n",
      "2025-11-02 13:36:56,936 - INFO - ============================================================\n",
      "2025-11-02 13:36:57,671 - INFO - Found 43 pages for beekman-holidays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "beekman-holidays: 100%|██████████| 43/43 [00:37<00:00,  1.15page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:37:35,056 - INFO -  Completed beekman-holidays: 467 total records\n",
      "2025-11-02 13:37:35,069 - INFO -  Checkpoint saved for beekman-holidays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:37:37,090 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:37:37,094 - INFO - API 3/17: one-world-travel-group\n",
      "2025-11-02 13:37:37,104 - INFO - ************************************************************\n",
      "2025-11-02 13:37:37,111 - INFO - ============================================================\n",
      "2025-11-02 13:37:37,121 - INFO - Processing API 3/17: one-world-travel-group\n",
      "2025-11-02 13:37:37,133 - INFO - ============================================================\n",
      "2025-11-02 13:37:37,779 - INFO - Found 15 pages for one-world-travel-group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "one-world-travel-group: 100%|██████████| 15/15 [00:12<00:00,  1.19page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:37:50,448 - INFO -  Completed one-world-travel-group: 164 total records\n",
      "2025-11-02 13:37:50,459 - INFO -  Checkpoint saved for one-world-travel-group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:37:52,477 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:37:52,483 - INFO - API 4/17: the-holiday-club\n",
      "2025-11-02 13:37:52,489 - INFO - ************************************************************\n",
      "2025-11-02 13:37:52,498 - INFO - ============================================================\n",
      "2025-11-02 13:37:52,504 - INFO - Processing API 4/17: the-holiday-club\n",
      "2025-11-02 13:37:52,511 - INFO - ============================================================\n",
      "2025-11-02 13:37:53,216 - INFO - Found 98 pages for the-holiday-club\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the-holiday-club: 100%|██████████| 98/98 [01:30<00:00,  1.08page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:39:24,084 - INFO -  Completed the-holiday-club: 1068 total records\n",
      "2025-11-02 13:39:24,094 - INFO -  Checkpoint saved for the-holiday-club\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:39:26,104 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:39:26,109 - INFO - API 5/17: thompsons-holidays\n",
      "2025-11-02 13:39:26,116 - INFO - ************************************************************\n",
      "2025-11-02 13:39:26,124 - INFO - ============================================================\n",
      "2025-11-02 13:39:26,130 - INFO - Processing API 5/17: thompsons-holidays\n",
      "2025-11-02 13:39:26,139 - INFO - ============================================================\n",
      "2025-11-02 13:39:26,941 - INFO - Found 30 pages for thompsons-holidays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thompsons-holidays: 100%|██████████| 30/30 [00:25<00:00,  1.16page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:39:52,875 - INFO -  Completed thompsons-holidays: 327 total records\n",
      "2025-11-02 13:39:52,887 - INFO -  Checkpoint saved for thompsons-holidays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:39:54,907 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:39:54,914 - INFO - API 6/17: iexchange\n",
      "2025-11-02 13:39:54,921 - INFO - ************************************************************\n",
      "2025-11-02 13:39:54,925 - INFO - ============================================================\n",
      "2025-11-02 13:39:54,929 - INFO - Processing API 6/17: iexchange\n",
      "2025-11-02 13:39:54,938 - INFO - ============================================================\n",
      "2025-11-02 13:39:55,624 - INFO - Found 14 pages for iexchange\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iexchange: 100%|██████████| 14/14 [00:15<00:00,  1.10s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:40:11,098 - INFO -  Completed iexchange: 150 total records\n",
      "2025-11-02 13:40:11,112 - INFO -  Checkpoint saved for iexchange\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:40:13,134 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:40:13,141 - INFO - API 7/17: lekkeslaapcoza\n",
      "2025-11-02 13:40:13,149 - INFO - ************************************************************\n",
      "2025-11-02 13:40:13,158 - INFO - ============================================================\n",
      "2025-11-02 13:40:13,164 - INFO - Processing API 7/17: lekkeslaapcoza\n",
      "2025-11-02 13:40:13,169 - INFO - ============================================================\n",
      "2025-11-02 13:40:13,879 - INFO - Found 88 pages for lekkeslaapcoza\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lekkeslaapcoza: 100%|██████████| 88/88 [01:20<00:00,  1.09page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:41:34,330 - INFO -  Completed lekkeslaapcoza: 964 total records\n",
      "2025-11-02 13:41:34,335 - INFO -  Checkpoint saved for lekkeslaapcoza\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:41:36,350 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:41:36,355 - INFO - API 8/17: the-beekman-group\n",
      "2025-11-02 13:41:36,365 - INFO - ************************************************************\n",
      "2025-11-02 13:41:36,371 - INFO - ============================================================\n",
      "2025-11-02 13:41:36,377 - INFO - Processing API 8/17: the-beekman-group\n",
      "2025-11-02 13:41:36,385 - INFO - ============================================================\n",
      "2025-11-02 13:41:37,091 - INFO - Found 23 pages for the-beekman-group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the-beekman-group: 100%|██████████| 23/23 [00:20<00:00,  1.11page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:41:57,790 - INFO -  Completed the-beekman-group: 253 total records\n",
      "2025-11-02 13:41:57,800 - INFO -  Checkpoint saved for the-beekman-group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:41:59,823 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:41:59,828 - INFO - API 9/17: flexi-club\n",
      "2025-11-02 13:41:59,834 - INFO - ************************************************************\n",
      "2025-11-02 13:41:59,842 - INFO - ============================================================\n",
      "2025-11-02 13:41:59,844 - INFO - Processing API 9/17: flexi-club\n",
      "2025-11-02 13:41:59,850 - INFO - ============================================================\n",
      "2025-11-02 13:42:00,695 - INFO - Found 66 pages for flexi-club\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flexi-club: 100%|██████████| 66/66 [01:01<00:00,  1.07page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:43:02,212 - INFO -  Completed flexi-club: 720 total records\n",
      "2025-11-02 13:43:02,219 - INFO -  Checkpoint saved for flexi-club\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:43:04,235 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:43:04,241 - INFO - API 10/17: flight-centre-travel-group\n",
      "2025-11-02 13:43:04,250 - INFO - ************************************************************\n",
      "2025-11-02 13:43:04,253 - INFO - ============================================================\n",
      "2025-11-02 13:43:04,260 - INFO - Processing API 10/17: flight-centre-travel-group\n",
      "2025-11-02 13:43:04,265 - INFO - ============================================================\n",
      "2025-11-02 13:43:05,137 - INFO - Found 201 pages for flight-centre-travel-group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flight-centre-travel-group: 100%|██████████| 201/201 [03:10<00:00,  1.06page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:46:15,226 - INFO -  Completed flight-centre-travel-group: 2203 total records\n",
      "2025-11-02 13:46:15,242 - INFO -  Checkpoint saved for flight-centre-travel-group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:46:17,253 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:46:17,258 - INFO - API 11/17: travelwingscom\n",
      "2025-11-02 13:46:17,269 - INFO - ************************************************************\n",
      "2025-11-02 13:46:17,276 - INFO - ============================================================\n",
      "2025-11-02 13:46:17,285 - INFO - Processing API 11/17: travelwingscom\n",
      "2025-11-02 13:46:17,291 - INFO - ============================================================\n",
      "2025-11-02 13:46:17,963 - INFO - Found 39 pages for travelwingscom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "travelwingscom: 100%|██████████| 39/39 [00:35<00:00,  1.11page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:46:53,008 - INFO -  Completed travelwingscom: 429 total records\n",
      "2025-11-02 13:46:53,019 - INFO -  Checkpoint saved for travelwingscom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:46:55,025 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:46:55,032 - INFO - API 12/17: easy-holidays\n",
      "2025-11-02 13:46:55,037 - INFO - ************************************************************\n",
      "2025-11-02 13:46:55,040 - INFO - ============================================================\n",
      "2025-11-02 13:46:55,046 - INFO - Processing API 12/17: easy-holidays\n",
      "2025-11-02 13:46:55,053 - INFO - ============================================================\n",
      "2025-11-02 13:46:55,758 - INFO - Found 79 pages for easy-holidays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "easy-holidays: 100%|██████████| 79/79 [01:08<00:00,  1.16page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:48:03,993 - INFO -  Completed easy-holidays: 862 total records\n",
      "2025-11-02 13:48:04,008 - INFO -  Checkpoint saved for easy-holidays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:48:06,027 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:48:06,032 - INFO - API 13/17: busbud\n",
      "2025-11-02 13:48:06,037 - INFO - ************************************************************\n",
      "2025-11-02 13:48:06,048 - INFO - ============================================================\n",
      "2025-11-02 13:48:06,058 - INFO - Processing API 13/17: busbud\n",
      "2025-11-02 13:48:06,063 - INFO - ============================================================\n",
      "2025-11-02 13:48:06,862 - INFO - Found 22 pages for busbud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "busbud: 100%|██████████| 22/22 [00:20<00:00,  1.06page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:48:27,623 - INFO -  Completed busbud: 238 total records\n",
      "2025-11-02 13:48:27,635 - INFO -  Checkpoint saved for busbud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:48:29,648 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:48:29,651 - INFO - API 14/17: forever-resorts\n",
      "2025-11-02 13:48:29,655 - INFO - ************************************************************\n",
      "2025-11-02 13:48:29,659 - INFO - ============================================================\n",
      "2025-11-02 13:48:29,661 - INFO - Processing API 14/17: forever-resorts\n",
      "2025-11-02 13:48:29,664 - INFO - ============================================================\n",
      "2025-11-02 13:48:30,393 - INFO - Found 16 pages for forever-resorts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "forever-resorts: 100%|██████████| 16/16 [00:14<00:00,  1.07page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:48:45,397 - INFO -  Completed forever-resorts: 175 total records\n",
      "2025-11-02 13:48:45,408 - INFO -  Checkpoint saved for forever-resorts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:48:47,419 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:48:47,426 - INFO - API 15/17: first-loyalty-plus\n",
      "2025-11-02 13:48:47,434 - INFO - ************************************************************\n",
      "2025-11-02 13:48:47,437 - INFO - ============================================================\n",
      "2025-11-02 13:48:47,442 - INFO - Processing API 15/17: first-loyalty-plus\n",
      "2025-11-02 13:48:47,456 - INFO - ============================================================\n",
      "2025-11-02 13:48:48,176 - INFO - Found 20 pages for first-loyalty-plus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "first-loyalty-plus: 100%|██████████| 20/20 [00:17<00:00,  1.13page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:49:05,840 - INFO -  Completed first-loyalty-plus: 214 total records\n",
      "2025-11-02 13:49:05,852 - INFO -  Checkpoint saved for first-loyalty-plus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:49:07,873 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:49:07,880 - INFO - API 16/17: southern-sun\n",
      "2025-11-02 13:49:07,886 - INFO - ************************************************************\n",
      "2025-11-02 13:49:07,896 - INFO - ============================================================\n",
      "2025-11-02 13:49:07,902 - INFO - Processing API 16/17: southern-sun\n",
      "2025-11-02 13:49:07,914 - INFO - ============================================================\n",
      "2025-11-02 13:49:08,975 - INFO - Found 67 pages for southern-sun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "southern-sun: 100%|██████████| 67/67 [01:03<00:00,  1.06page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:12,527 - INFO -  Completed southern-sun: 728 total records\n",
      "2025-11-02 13:50:12,551 - INFO -  Checkpoint saved for southern-sun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:14,557 - INFO - \n",
      "************************************************************\n",
      "2025-11-02 13:50:14,563 - INFO - API 17/17: campuskey\n",
      "2025-11-02 13:50:14,572 - INFO - ************************************************************\n",
      "2025-11-02 13:50:14,583 - INFO - ============================================================\n",
      "2025-11-02 13:50:14,589 - INFO - Processing API 17/17: campuskey\n",
      "2025-11-02 13:50:14,596 - INFO - ============================================================\n",
      "2025-11-02 13:50:15,362 - INFO - Found 6 pages for campuskey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "campuskey: 100%|██████████| 6/6 [00:05<00:00,  1.03page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:21,223 - INFO -  Completed campuskey: 61 total records\n",
      "2025-11-02 13:50:21,244 - INFO -  Checkpoint saved for campuskey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:23,312 - INFO - \n",
      "############################################################\n",
      "2025-11-02 13:50:23,319 - INFO - PIPELINE COMPLETED\n",
      "2025-11-02 13:50:23,327 - INFO - Total time: 947.45 seconds (15.79 minutes)\n",
      "2025-11-02 13:50:23,331 - INFO - Total records: 10602\n",
      "2025-11-02 13:50:23,342 - INFO - Completed APIs: 17\n",
      "2025-11-02 13:50:23,349 - INFO - Failed APIs: 0\n",
      "2025-11-02 13:50:23,364 - INFO - ############################################################\n",
      "\n",
      "============================================================\n",
      "DATA COLLECTION COMPLETED\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Execute Pipeline\n",
    "Run the main data collection pipeline.\n",
    "This cell executes the asynchronous pipeline.\n",
    "\"\"\"\n",
    "\n",
    "# Run the pipeline\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING DATA COLLECTION PIPELINE\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Execute async pipeline\n",
    "await run_pipeline()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA COLLECTION COMPLETED\")\n",
    "print(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da841a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n##This will continue from where it left off\\n# Check current progress\\nprint(f\"Completed: {len(checkpoint.state[\\'completed_apis\\'])} APIs\")\\nprint(f\"Failed: {len(checkpoint.state[\\'failed_apis\\'])} APIs\")\\nprint(f\"Current position: {checkpoint.get_resume_index()}/{len(INSURANCE_APIS)}\")\\nprint(f\"Total records so far: {checkpoint.state[\\'total_records_processed\\']:,}\")\\n\\n# Then resume\\nawait resume_pipeline()\\n'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "##This will continue from where it left off\n",
    "# Check current progress\n",
    "print(f\"Completed: {len(checkpoint.state['completed_apis'])} APIs\")\n",
    "print(f\"Failed: {len(checkpoint.state['failed_apis'])} APIs\")\n",
    "print(f\"Current position: {checkpoint.get_resume_index()}/{len(INSURANCE_APIS)}\")\n",
    "print(f\"Total records so far: {checkpoint.state['total_records_processed']:,}\")\n",
    "\n",
    "# Then resume\n",
    "await resume_pipeline()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d33b5",
   "metadata": {},
   "source": [
    "## Consolidate Data from Temporary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc27ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:23,674 - INFO - Starting data consolidation...\n",
      "\n",
      "============================================================\n",
      "CONSOLIDATING DATA\n",
      "============================================================\n",
      "\n",
      "2025-11-02 13:50:26,694 - INFO -  Wrote header and data for pentravel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:27,474 - INFO -  Appended 467 records for beekman-holidays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating:   6%|▋         | 1/16 [00:00<00:11,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:27,734 - INFO -  Appended 164 records for one-world-travel-group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating:  12%|█▎        | 2/16 [00:01<00:06,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:28,734 - INFO -  Appended 1068 records for the-holiday-club\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating:  19%|█▉        | 3/16 [00:02<00:09,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:29,217 - INFO -  Appended 327 records for thompsons-holidays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating:  25%|██▌       | 4/16 [00:02<00:07,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:29,542 - INFO -  Appended 150 records for iexchange\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating:  31%|███▏      | 5/16 [00:02<00:05,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:30,307 - INFO -  Appended 964 records for lekkeslaapcoza\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating:  38%|███▊      | 6/16 [00:03<00:06,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:30,604 - INFO -  Appended 253 records for the-beekman-group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating:  44%|████▍     | 7/16 [00:03<00:04,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:31,352 - INFO -  Appended 720 records for flexi-club\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating:  50%|█████     | 8/16 [00:04<00:04,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:33,416 - INFO -  Appended 2203 records for flight-centre-travel-group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating:  56%|█████▋    | 9/16 [00:06<00:07,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:33,796 - INFO -  Appended 429 records for travelwingscom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating:  62%|██████▎   | 10/16 [00:07<00:05,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:34,696 - INFO -  Appended 862 records for easy-holidays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating:  69%|██████▉   | 11/16 [00:07<00:04,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:34,947 - INFO -  Appended 238 records for busbud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating:  75%|███████▌  | 12/16 [00:08<00:02,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:35,252 - INFO -  Appended 175 records for forever-resorts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating:  81%|████████▏ | 13/16 [00:08<00:01,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:35,502 - INFO -  Appended 214 records for first-loyalty-plus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating:  88%|████████▊ | 14/16 [00:08<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:36,062 - INFO -  Appended 728 records for southern-sun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating:  94%|█████████▍| 15/16 [00:09<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:36,261 - INFO -  Appended 61 records for campuskey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating: 100%|██████████| 16/16 [00:09<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:50:36,502 - INFO - \n",
      "============================================================\n",
      "2025-11-02 13:50:36,509 - INFO - CONSOLIDATION COMPLETE\n",
      "2025-11-02 13:50:36,518 - INFO - Output file: customer_travel_reviews_final.csv\n",
      "2025-11-02 13:50:36,525 - INFO - File size: 17.73 MB\n",
      "2025-11-02 13:50:36,531 - INFO - Total businesses: 17\n",
      "2025-11-02 13:50:36,534 - INFO - ============================================================\n",
      "\n",
      " Final dataset saved to: customer_travel_reviews_final.csv\n",
      " File size: 17.73 MB\n",
      " Total businesses processed: 17\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Consolidate Data\n",
    "Merge all temporary JSON chunks into a single CSV file.\n",
    "This step processes data in batches to minimize memory usage.\n",
    "\"\"\"\n",
    "\n",
    "def consolidate_all_data():\n",
    "    \"\"\"\n",
    "    Consolidate all temporary data files into final CSV\n",
    "    Uses chunked processing to minimize memory usage\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting data consolidation...\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONSOLIDATING DATA\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    final_output = Path(CONFIG['FINAL_OUTPUT'])\n",
    "    temp_dir = Path(CONFIG['TEMP_DIR'])\n",
    "    \n",
    "    # Get all unique business names from checkpoint\n",
    "    completed_apis = [api['name'] for api in checkpoint.state['completed_apis']]\n",
    "    \n",
    "    if not completed_apis:\n",
    "        logger.warning(\"No completed APIs found to consolidate\")\n",
    "        return\n",
    "    \n",
    "    # Write header\n",
    "    first_df = load_temp_files_for_business(completed_apis[0])\n",
    "    first_df.to_csv(final_output, index=False, mode='w')\n",
    "    logger.info(f\" Wrote header and data for {completed_apis[0]}\")\n",
    "    \n",
    "    # Append remaining businesses\n",
    "    for business_name in tqdm(completed_apis[1:], desc=\"Consolidating\"):\n",
    "        try:\n",
    "            df = load_temp_files_for_business(business_name)\n",
    "            if not df.empty:\n",
    "                df.to_csv(final_output, index=False, mode='a', header=False)\n",
    "                logger.info(f\" Appended {len(df)} records for {business_name}\")\n",
    "            else:\n",
    "                logger.warning(f\"No data found for {business_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to consolidate {business_name}: {e}\")\n",
    "    \n",
    "    # Get final file size and record count\n",
    "    final_df = pd.read_csv(final_output, nrows=5)  # Sample to verify\n",
    "    file_size_mb = final_output.stat().st_size / (1024 * 1024)\n",
    "    \n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"CONSOLIDATION COMPLETE\")\n",
    "    logger.info(f\"Output file: {final_output}\")\n",
    "    logger.info(f\"File size: {file_size_mb:.2f} MB\")\n",
    "    logger.info(f\"Total businesses: {len(completed_apis)}\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\n Final dataset saved to: {CONFIG['FINAL_OUTPUT']}\")\n",
    "    print(f\" File size: {file_size_mb:.2f} MB\")\n",
    "    print(f\" Total businesses processed: {len(completed_apis)}\")\n",
    "\n",
    "# Run consolidation\n",
    "consolidate_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede321c6",
   "metadata": {},
   "source": [
    "## Data Verification and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VERIFYING FINAL OUTPUT\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miehl\\AppData\\Local\\Temp\\ipykernel_27432\\3416397849.py:19: DtypeWarning: Columns (22,32,41,61,66,82,87,93,101,105,106,107,108,109,110) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in chunk_iterator:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total records: 10,602\n",
      " Unique businesses: 1\n",
      "\n",
      "Top 10 businesses by review count:\n",
      "------------------------------------------------------------\n",
      "  pentravel                                     1,579 reviews\n",
      "\n",
      "============================================================\n",
      "SAMPLE RECORDS (first 5 rows)\n",
      "============================================================\n",
      "\n",
      "        id                               user_id           created_at authorDisplayName      author  authorAvatar                             author_id                                              review_title  review_rating                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            review_content business_name business_slug                                                         permalink  replied                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        messages                                                                 business_logo                                 industry_logo      industry_name    industry_slug  status_id  nps_rating    source  is_reported  business_reporting author_created_date  author_total_reviews_count attachments consumerCompetitions  source_id  org_id  platform_id  review_type_id  slug  review_image  review_image_thumb  location_lat  location_lng  city  state  postal  response_received  response_timestamp  time_to_respond  converted  converted_rating  rating_change  final_rating  inclination  supercharged_request  supercharged_active  help_requested  review_reported  review_blacklisted  review_visible  shared_facebook  shared_twitter  facebook_message  twitter_message  author_contact  additional_received  incident_ref_number  incident_id_number  incident_account_number  incident_contact_person  incident_contact_email  incident_contact_number  incident_branch  incident_timestamp  org_rating  org_nature  org_department  org_branch  org_root_cause  org_action  org_person_responsible  org_implemented  feedback_rating  feedback_helpful  feedback_remark  old_url  old_org_id  industryId  updated_at  orgdata_received  user_submitted  promoted  assigned_to  ip_address  feedback_needed  impression_count  view_count  counted  fetched  device  reassigned  delete_reason  starred  message_count  message_counted  updated  notes  notes_datetime  conversion_reason  read  delete_biz_reason  business  effective_consumer_competitions  nps  review_source  reports Business_Name\n",
      "0  6030386  f97933c0-d40c-11ef-a23d-351d04a973c7  2025-10-31 16:41:31         Winston S   Winston S           NaN  f97933c0-d40c-11ef-a23d-351d04a973c7                                   Pentravel: Good service              5                                                                                                                                                                                                                                                                                          During the planning of our international trip I kept on making adjustments.  The final plan does not come close to the initial plan proposed by Pentravel. Lindsay Ransom, from the online section was extremely patient, provided excellent advice and guidance. Lindsay put significant time and effort in to ensure that what I had in mind can be executed.\\n\\nWell Lindsay; I will contact you soon for my next international trip\\n\\nWinston Smit, Tshwane     Pentravel     pentravel                                    pentravel-good-service-6030386        1                  [{'id': 4126805, 'body': \"<div>Dear Winston Smit<br><br></div><div>Thank you so much for taking the time to acknowledge the excellent service received from Lindsay Ransom at our Online Branch.</div><div><br></div><div>Pentravel is a business that prides itself in great service delivery and takes our pledge of 'where customers love doing business' very seriously.&nbsp; It's always great to receive feedback to ensure we are doing just that, and we look forward to assisting with many more future travel arrangements.<br><br>Your feedback will be shared with all at Pentravel to ensure that Lindsay receives the recognition she deserves.<br><br>Kind regards,<br>The Pentravel Team&nbsp;</div>\", 'created_at': '2025-10-31 17:28:56', 'type': 'Public', 'origin': 1, 'owner': 2, 'from': 'Pentravel', 'businessSlug': 'pentravel', 'businessLogo': 'https://s3-us-west-2.amazonaws.com/hp-cdn-01/uploads/orgs/pentravel_logo.jpg', 'showPrivateBadge': False, 'isLinkedToUser': False}]  https://s3-us-west-2.amazonaws.com/hp-cdn-01/uploads/orgs/pentravel_logo.jpg  /static/img/industries/icons/travel-icon.jpg  Travel & Vacation  travel-vacation          1         NaN   WEBSITE        False                 NaN          2025-01-16                         2.0          []                   []        NaN     NaN          NaN             NaN   NaN           NaN                 NaN           NaN           NaN   NaN    NaN     NaN                NaN                 NaN              NaN        NaN               NaN            NaN           NaN          NaN                   NaN                  NaN             NaN              NaN                 NaN             NaN              NaN             NaN               NaN              NaN             NaN                  NaN                  NaN                 NaN                      NaN                      NaN                     NaN                      NaN              NaN                 NaN         NaN         NaN             NaN         NaN             NaN         NaN                     NaN              NaN              NaN               NaN              NaN      NaN         NaN         NaN         NaN               NaN             NaN       NaN          NaN         NaN              NaN               NaN         NaN      NaN      NaN     NaN         NaN            NaN      NaN            NaN              NaN      NaN    NaN             NaN                NaN   NaN                NaN       NaN                              NaN  NaN            NaN      NaN     pentravel\n",
      "1  6022970  ba185d40-b2c7-11ed-9902-b97713e52ebf  2025-10-28 14:23:01           Nancy F     Nancy F           NaN  ba185d40-b2c7-11ed-9902-b97713e52ebf                                   I booked flights wit...              5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                I booked flights with Lopang Mogari at Clearwater Pentravel - she is a star - warm, caring, friendly, and so efficient - great experience.     Pentravel     pentravel                                      i-booked-flights-wit-6022970        1           [{'id': 4120562, 'body': \"<div>Dear Nancy Frank<br><br></div><div>Thank you so much for taking the time to acknowledge the friendly and efficient service received from Lopang at our Clearwater Branch.</div><div><br></div><div>Pentravel is a business that prides itself in great service delivery and takes our pledge of 'where customers love doing business' very seriously.&nbsp; It's always great to receive feedback to ensure we are doing just that, and we look forward to assisting with many more future travel arrangements.<br><br>Your feedback will be shared with all at Pentravel to ensure that Lopang receives the recognition she deserves.<br><br>Kind regards,<br>The Pentravel Team&nbsp;</div>\", 'created_at': '2025-10-28 18:26:12', 'type': 'Public', 'origin': 1, 'owner': 2, 'from': 'Pentravel', 'businessSlug': 'pentravel', 'businessLogo': 'https://s3-us-west-2.amazonaws.com/hp-cdn-01/uploads/orgs/pentravel_logo.jpg', 'showPrivateBadge': False, 'isLinkedToUser': False}]  https://s3-us-west-2.amazonaws.com/hp-cdn-01/uploads/orgs/pentravel_logo.jpg  /static/img/industries/icons/travel-icon.jpg  Travel & Vacation  travel-vacation          1         NaN  WHATSAPP        False                 NaN          2023-02-22                         5.0          []                   []        NaN     NaN          NaN             NaN   NaN           NaN                 NaN           NaN           NaN   NaN    NaN     NaN                NaN                 NaN              NaN        NaN               NaN            NaN           NaN          NaN                   NaN                  NaN             NaN              NaN                 NaN             NaN              NaN             NaN               NaN              NaN             NaN                  NaN                  NaN                 NaN                      NaN                      NaN                     NaN                      NaN              NaN                 NaN         NaN         NaN             NaN         NaN             NaN         NaN                     NaN              NaN              NaN               NaN              NaN      NaN         NaN         NaN         NaN               NaN             NaN       NaN          NaN         NaN              NaN               NaN         NaN      NaN      NaN     NaN         NaN            NaN      NaN            NaN              NaN      NaN    NaN             NaN                NaN   NaN                NaN       NaN                              NaN  NaN            NaN      NaN     pentravel\n",
      "2  6020347  6b39fdb0-b321-11f0-ac22-ad5bad6f3720  2025-10-27 12:51:54        Thembeka M  Thembeka M           NaN  6b39fdb0-b321-11f0-ac22-ad5bad6f3720  Exceptional Service and an Unforgettable Bali Experience              5  We had the most wonderful experience booking our holiday through Tegan. Everything was perfectly organized, from the flights and accommodation to the airport assistance for my mom, which made the trip so smooth and comfortable.\\n\\nOur stay in Bali has been absolutely amazing. The place Tegan booked for us was perfect, the food has been fantastic, and we’ve had such a great time exploring. Communication throughout has been excellent, and every detail has been thoughtfully taken care of.\\n\\nThank you, Tegan, for making this such a memorable and stress-free experience for us. We truly appreciate your professionalism, attention to detail, and kindness. Highly recommend your services to anyone planning their next getaway :)     Pentravel     pentravel  exceptional-service-and-an-unforgettable-bali-experience-6020347        1           [{'id': 4117769, 'body': \"<div>Dear Thembeka Meyiwa<br><br></div><div>Thank you so much for taking the time to acknowledge the kind and professional service received from Tegan at our Hillcrest Branch.</div><div><br></div><div>Pentravel is a business that prides itself in great service delivery and takes our pledge of 'where customers love doing business' very seriously.&nbsp; It's always great to receive feedback to ensure we are doing just that, and we look forward to assisting with many more future travel arrangements.<br><br>Your feedback will be shared with all at Pentravel to ensure that Tegan receives the recognition she deserves.<br><br>Kind regards,<br>The Pentravel Team&nbsp;</div>\", 'created_at': '2025-10-27 13:55:11', 'type': 'Public', 'origin': 1, 'owner': 2, 'from': 'Pentravel', 'businessSlug': 'pentravel', 'businessLogo': 'https://s3-us-west-2.amazonaws.com/hp-cdn-01/uploads/orgs/pentravel_logo.jpg', 'showPrivateBadge': False, 'isLinkedToUser': False}]  https://s3-us-west-2.amazonaws.com/hp-cdn-01/uploads/orgs/pentravel_logo.jpg  /static/img/industries/icons/travel-icon.jpg  Travel & Vacation  travel-vacation          1         9.0   WEBSITE        False                 NaN          2025-10-27                         1.0          []                   []        NaN     NaN          NaN             NaN   NaN           NaN                 NaN           NaN           NaN   NaN    NaN     NaN                NaN                 NaN              NaN        NaN               NaN            NaN           NaN          NaN                   NaN                  NaN             NaN              NaN                 NaN             NaN              NaN             NaN               NaN              NaN             NaN                  NaN                  NaN                 NaN                      NaN                      NaN                     NaN                      NaN              NaN                 NaN         NaN         NaN             NaN         NaN             NaN         NaN                     NaN              NaN              NaN               NaN              NaN      NaN         NaN         NaN         NaN               NaN             NaN       NaN          NaN         NaN              NaN               NaN         NaN      NaN      NaN     NaN         NaN            NaN      NaN            NaN              NaN      NaN    NaN             NaN                NaN   NaN                NaN       NaN                              NaN  NaN            NaN      NaN     pentravel\n",
      "3  6016277  0ad34e3e-31fa-11e8-83f4-f23c91bb6188  2025-10-24 08:56:01           CHESTAN     CHESTAN           NaN  0ad34e3e-31fa-11e8-83f4-f23c91bb6188                        Brilliant Service and Professional              5                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Wow all I can say, So professional TEGAN H. went out of way to assist even being over ****ms away. she assisted in booking and making sure all our travel arrangement were made and confirmed. I and my company will definately make use of Pentravel and Tegan again for all our travel.     Pentravel     pentravel                        brilliant-service-and-professional-6016277        1                   [{'id': 4114087, 'body': \"<div>Dear Chestan Meadon<br><br></div><div>Thank you so much for taking the time to acknowledge the professional service received from Teagan at our Hillcrest Branch.</div><div><br></div><div>Pentravel is a business that prides itself in great service delivery and takes our pledge of 'where customers love doing business' very seriously.&nbsp; It's always great to receive feedback to ensure we are doing just that, and we look forward to assisting with many more future travel arrangements.<br><br>Your feedback will be shared with all at Pentravel to ensure that Teagan receives the recognition she deserves.<br><br>Kind regards,<br>The Pentravel Team&nbsp;</div>\", 'created_at': '2025-10-24 08:58:40', 'type': 'Public', 'origin': 1, 'owner': 2, 'from': 'Pentravel', 'businessSlug': 'pentravel', 'businessLogo': 'https://s3-us-west-2.amazonaws.com/hp-cdn-01/uploads/orgs/pentravel_logo.jpg', 'showPrivateBadge': False, 'isLinkedToUser': False}]  https://s3-us-west-2.amazonaws.com/hp-cdn-01/uploads/orgs/pentravel_logo.jpg  /static/img/industries/icons/travel-icon.jpg  Travel & Vacation  travel-vacation          1         NaN   WEBSITE        False                 NaN          2012-05-28                         4.0          []                   []        NaN     NaN          NaN             NaN   NaN           NaN                 NaN           NaN           NaN   NaN    NaN     NaN                NaN                 NaN              NaN        NaN               NaN            NaN           NaN          NaN                   NaN                  NaN             NaN              NaN                 NaN             NaN              NaN             NaN               NaN              NaN             NaN                  NaN                  NaN                 NaN                      NaN                      NaN                     NaN                      NaN              NaN                 NaN         NaN         NaN             NaN         NaN             NaN         NaN                     NaN              NaN              NaN               NaN              NaN      NaN         NaN         NaN         NaN               NaN             NaN       NaN          NaN         NaN              NaN               NaN         NaN      NaN      NaN     NaN         NaN            NaN      NaN            NaN              NaN      NaN    NaN             NaN                NaN   NaN                NaN       NaN                              NaN  NaN            NaN      NaN     pentravel\n",
      "4  6013558  78a57b50-a9cc-11f0-8b94-51d9f89bd38b  2025-10-22 17:36:08         Zezethu M   Zezethu M           NaN  78a57b50-a9cc-11f0-8b94-51d9f89bd38b                                     Preanca Niadoo review              5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Preanca was so amazing and helped me so much, so excited to go on my holiday     Pentravel     pentravel                                     preanca-niadoo-review-6013558        1  [{'id': 4111781, 'body': \"<div>Dear Zezethu Matyolo<br><br></div><div>Thank you so much for taking the time to acknowledge the helpful service received from Preanca at our Hillcrest Branch.</div><div><br></div><div>Pentravel is a business that prides itself in great service delivery and takes our pledge of 'where customers love doing business' very seriously.&nbsp; It's always great to receive feedback to ensure we are doing just that, and we look forward to assisting with many more future travel arrangements.<br><br>Your feedback will be shared with all at Pentravel to ensure that Preanca receives the recognition she deserves.<br><br>Kind regards,<br>The Pentravel Team&nbsp;</div><div><br><br></div>\", 'created_at': '2025-10-22 19:07:29', 'type': 'Public', 'origin': 1, 'owner': 2, 'from': 'Pentravel', 'businessSlug': 'pentravel', 'businessLogo': 'https://s3-us-west-2.amazonaws.com/hp-cdn-01/uploads/orgs/pentravel_logo.jpg', 'showPrivateBadge': False, 'isLinkedToUser': False}]  https://s3-us-west-2.amazonaws.com/hp-cdn-01/uploads/orgs/pentravel_logo.jpg  /static/img/industries/icons/travel-icon.jpg  Travel & Vacation  travel-vacation          1         NaN   WEBSITE        False                 NaN          2025-10-15                         2.0          []                   []        NaN     NaN          NaN             NaN   NaN           NaN                 NaN           NaN           NaN   NaN    NaN     NaN                NaN                 NaN              NaN        NaN               NaN            NaN           NaN          NaN                   NaN                  NaN             NaN              NaN                 NaN             NaN              NaN             NaN               NaN              NaN             NaN                  NaN                  NaN                 NaN                      NaN                      NaN                     NaN                      NaN              NaN                 NaN         NaN         NaN             NaN         NaN             NaN         NaN                     NaN              NaN              NaN               NaN              NaN      NaN         NaN         NaN         NaN               NaN             NaN       NaN          NaN         NaN              NaN               NaN         NaN      NaN      NaN     NaN         NaN            NaN      NaN            NaN              NaN      NaN    NaN             NaN                NaN   NaN                NaN       NaN                              NaN  NaN            NaN      NaN     pentravel\n",
      "\n",
      "============================================================\n",
      "DATASET INFORMATION\n",
      "============================================================\n",
      "\n",
      "\n",
      "Column Names and Types:\n",
      "------------------------------------------------------------\n",
      "  id                             int64           (0 nulls in sample)\n",
      "  user_id                        object          (0 nulls in sample)\n",
      "  created_at                     object          (0 nulls in sample)\n",
      "  authorDisplayName              object          (0 nulls in sample)\n",
      "  author                         object          (0 nulls in sample)\n",
      "  authorAvatar                   float64         (1000 nulls in sample)\n",
      "  author_id                      object          (0 nulls in sample)\n",
      "  review_title                   object          (0 nulls in sample)\n",
      "  review_rating                  int64           (0 nulls in sample)\n",
      "  review_content                 object          (0 nulls in sample)\n",
      "  business_name                  object          (0 nulls in sample)\n",
      "  business_slug                  object          (0 nulls in sample)\n",
      "  permalink                      object          (0 nulls in sample)\n",
      "  replied                        int64           (0 nulls in sample)\n",
      "  messages                       object          (0 nulls in sample)\n",
      "  business_logo                  object          (0 nulls in sample)\n",
      "  industry_logo                  object          (0 nulls in sample)\n",
      "  industry_name                  object          (0 nulls in sample)\n",
      "  industry_slug                  object          (0 nulls in sample)\n",
      "  status_id                      int64           (0 nulls in sample)\n",
      "  nps_rating                     float64         (773 nulls in sample)\n",
      "  source                         object          (279 nulls in sample)\n",
      "  is_reported                    bool            (0 nulls in sample)\n",
      "  business_reporting             object          (995 nulls in sample)\n",
      "  author_created_date            object          (0 nulls in sample)\n",
      "  author_total_reviews_count     float64         (0 nulls in sample)\n",
      "  attachments                    object          (0 nulls in sample)\n",
      "  consumerCompetitions           object          (0 nulls in sample)\n",
      "  source_id                      float64         (1000 nulls in sample)\n",
      "  org_id                         float64         (1000 nulls in sample)\n",
      "  platform_id                    float64         (1000 nulls in sample)\n",
      "  review_type_id                 float64         (1000 nulls in sample)\n",
      "  slug                           float64         (1000 nulls in sample)\n",
      "  review_image                   float64         (1000 nulls in sample)\n",
      "  review_image_thumb             float64         (1000 nulls in sample)\n",
      "  location_lat                   float64         (1000 nulls in sample)\n",
      "  location_lng                   float64         (1000 nulls in sample)\n",
      "  city                           float64         (1000 nulls in sample)\n",
      "  state                          float64         (1000 nulls in sample)\n",
      "  postal                         float64         (1000 nulls in sample)\n",
      "  response_received              float64         (1000 nulls in sample)\n",
      "  response_timestamp             float64         (1000 nulls in sample)\n",
      "  time_to_respond                float64         (1000 nulls in sample)\n",
      "  converted                      float64         (1000 nulls in sample)\n",
      "  converted_rating               float64         (1000 nulls in sample)\n",
      "  rating_change                  float64         (1000 nulls in sample)\n",
      "  final_rating                   float64         (1000 nulls in sample)\n",
      "  inclination                    float64         (1000 nulls in sample)\n",
      "  supercharged_request           float64         (1000 nulls in sample)\n",
      "  supercharged_active            float64         (1000 nulls in sample)\n",
      "  help_requested                 float64         (1000 nulls in sample)\n",
      "  review_reported                float64         (1000 nulls in sample)\n",
      "  review_blacklisted             float64         (1000 nulls in sample)\n",
      "  review_visible                 float64         (1000 nulls in sample)\n",
      "  shared_facebook                float64         (1000 nulls in sample)\n",
      "  shared_twitter                 float64         (1000 nulls in sample)\n",
      "  facebook_message               float64         (1000 nulls in sample)\n",
      "  twitter_message                float64         (1000 nulls in sample)\n",
      "  author_contact                 float64         (1000 nulls in sample)\n",
      "  additional_received            float64         (1000 nulls in sample)\n",
      "  incident_ref_number            float64         (1000 nulls in sample)\n",
      "  incident_id_number             float64         (1000 nulls in sample)\n",
      "  incident_account_number        float64         (1000 nulls in sample)\n",
      "  incident_contact_person        float64         (1000 nulls in sample)\n",
      "  incident_contact_email         float64         (1000 nulls in sample)\n",
      "  incident_contact_number        float64         (1000 nulls in sample)\n",
      "  incident_branch                float64         (1000 nulls in sample)\n",
      "  incident_timestamp             float64         (1000 nulls in sample)\n",
      "  org_rating                     float64         (1000 nulls in sample)\n",
      "  org_nature                     float64         (1000 nulls in sample)\n",
      "  org_department                 float64         (1000 nulls in sample)\n",
      "  org_branch                     float64         (1000 nulls in sample)\n",
      "  org_root_cause                 float64         (1000 nulls in sample)\n",
      "  org_action                     float64         (1000 nulls in sample)\n",
      "  org_person_responsible         float64         (1000 nulls in sample)\n",
      "  org_implemented                float64         (1000 nulls in sample)\n",
      "  feedback_rating                float64         (1000 nulls in sample)\n",
      "  feedback_helpful               float64         (1000 nulls in sample)\n",
      "  feedback_remark                float64         (1000 nulls in sample)\n",
      "  old_url                        float64         (1000 nulls in sample)\n",
      "  old_org_id                     float64         (1000 nulls in sample)\n",
      "  industryId                     float64         (1000 nulls in sample)\n",
      "  updated_at                     float64         (1000 nulls in sample)\n",
      "  orgdata_received               float64         (1000 nulls in sample)\n",
      "  user_submitted                 float64         (1000 nulls in sample)\n",
      "  promoted                       float64         (1000 nulls in sample)\n",
      "  assigned_to                    float64         (1000 nulls in sample)\n",
      "  ip_address                     float64         (1000 nulls in sample)\n",
      "  feedback_needed                float64         (1000 nulls in sample)\n",
      "  impression_count               float64         (1000 nulls in sample)\n",
      "  view_count                     float64         (1000 nulls in sample)\n",
      "  counted                        float64         (1000 nulls in sample)\n",
      "  fetched                        float64         (1000 nulls in sample)\n",
      "  device                         float64         (1000 nulls in sample)\n",
      "  reassigned                     float64         (1000 nulls in sample)\n",
      "  delete_reason                  float64         (1000 nulls in sample)\n",
      "  starred                        float64         (1000 nulls in sample)\n",
      "  message_count                  float64         (1000 nulls in sample)\n",
      "  message_counted                float64         (1000 nulls in sample)\n",
      "  updated                        float64         (1000 nulls in sample)\n",
      "  notes                          float64         (1000 nulls in sample)\n",
      "  notes_datetime                 float64         (1000 nulls in sample)\n",
      "  conversion_reason              float64         (1000 nulls in sample)\n",
      "  read                           float64         (1000 nulls in sample)\n",
      "  delete_biz_reason              float64         (1000 nulls in sample)\n",
      "  business                       float64         (1000 nulls in sample)\n",
      "  effective_consumer_competitions float64         (1000 nulls in sample)\n",
      "  nps                            float64         (1000 nulls in sample)\n",
      "  review_source                  float64         (1000 nulls in sample)\n",
      "  reports                        float64         (1000 nulls in sample)\n",
      "  Business_Name                  object          (0 nulls in sample)\n",
      "\n",
      " Data verification complete!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Verify Final Output\n",
    "Load and verify the consolidated dataset.\n",
    "Display summary statistics and sample records.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFYING FINAL OUTPUT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    # Load with chunking to avoid memory issues\n",
    "    chunk_iterator = pd.read_csv(CONFIG['FINAL_OUTPUT'], chunksize=10000)\n",
    "    \n",
    "    # Get basic stats without loading full dataset\n",
    "    total_rows = 0\n",
    "    business_counts = {}\n",
    "    \n",
    "    for chunk in chunk_iterator:\n",
    "        total_rows += len(chunk)\n",
    "        for business in chunk['Business_Name'].value_counts().items():\n",
    "            business_counts[business[0]] = business_counts.get(business[0], 0) + business[1]\n",
    "    \n",
    "    print(f\" Total records: {total_rows:,}\")\n",
    "    print(f\" Unique businesses: {len(business_counts)}\")\n",
    "    print(f\"\\nTop 10 businesses by review count:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    sorted_businesses = sorted(business_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for business, count in sorted_businesses:\n",
    "        print(f\"  {business:<40} {count:>10,} reviews\")\n",
    "    \n",
    "    # Display sample records (first 5 rows)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SAMPLE RECORDS (first 5 rows)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    sample_df = pd.read_csv(CONFIG['FINAL_OUTPUT'], nrows=5)\n",
    "    print(sample_df.to_string())\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DATASET INFORMATION\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Get column information without loading full dataset\n",
    "    full_df_sample = pd.read_csv(CONFIG['FINAL_OUTPUT'], nrows=1000)\n",
    "    print(\"\\nColumn Names and Types:\")\n",
    "    print(\"-\" * 60)\n",
    "    for col, dtype in full_df_sample.dtypes.items():\n",
    "        null_count = full_df_sample[col].isnull().sum()\n",
    "        print(f\"  {col:<30} {str(dtype):<15} ({null_count} nulls in sample)\")\n",
    "    \n",
    "    print(f\"\\n Data verification complete!\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\" Error: Output file not found at {CONFIG['FINAL_OUTPUT']}\")\n",
    "except Exception as e:\n",
    "    print(f\" Error during verification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a6ed1",
   "metadata": {},
   "source": [
    "## Pipeline Statistics and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2db7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PIPELINE EXECUTION REPORT\n",
      "============================================================\n",
      "\n",
      "EXECUTION SUMMARY\n",
      "------------------------------------------------------------\n",
      "Start Time:              2025-11-02T13:34:35.550394\n",
      "Last Update:             2025-11-02T13:50:21.236741\n",
      "Total Records Processed: 10,602\n",
      "Total APIs:              17\n",
      "Completed APIs:          17\n",
      "Failed APIs:             0\n",
      "\n",
      "============================================================\n",
      "COMPLETED APIS\n",
      "============================================================\n",
      "\n",
      "  pentravel\n",
      "    Records:   1,579\n",
      "    Completed: 2025-11-02T13:36:54.883770\n",
      "\n",
      "  beekman-holidays\n",
      "    Records:   467\n",
      "    Completed: 2025-11-02T13:37:35.064296\n",
      "\n",
      "  one-world-travel-group\n",
      "    Records:   164\n",
      "    Completed: 2025-11-02T13:37:50.457574\n",
      "\n",
      "  the-holiday-club\n",
      "    Records:   1,068\n",
      "    Completed: 2025-11-02T13:39:24.089707\n",
      "\n",
      "  thompsons-holidays\n",
      "    Records:   327\n",
      "    Completed: 2025-11-02T13:39:52.882171\n",
      "\n",
      "  iexchange\n",
      "    Records:   150\n",
      "    Completed: 2025-11-02T13:40:11.108990\n",
      "\n",
      "  lekkeslaapcoza\n",
      "    Records:   964\n",
      "    Completed: 2025-11-02T13:41:34.333935\n",
      "\n",
      "  the-beekman-group\n",
      "    Records:   253\n",
      "    Completed: 2025-11-02T13:41:57.794815\n",
      "\n",
      "  flexi-club\n",
      "    Records:   720\n",
      "    Completed: 2025-11-02T13:43:02.217251\n",
      "\n",
      "  flight-centre-travel-group\n",
      "    Records:   2,203\n",
      "    Completed: 2025-11-02T13:46:15.238436\n",
      "\n",
      "  travelwingscom\n",
      "    Records:   429\n",
      "    Completed: 2025-11-02T13:46:53.016092\n",
      "\n",
      "  easy-holidays\n",
      "    Records:   862\n",
      "    Completed: 2025-11-02T13:48:04.003307\n",
      "\n",
      "  busbud\n",
      "    Records:   238\n",
      "    Completed: 2025-11-02T13:48:27.632177\n",
      "\n",
      "  forever-resorts\n",
      "    Records:   175\n",
      "    Completed: 2025-11-02T13:48:45.404444\n",
      "\n",
      "  first-loyalty-plus\n",
      "    Records:   214\n",
      "    Completed: 2025-11-02T13:49:05.847019\n",
      "\n",
      "  southern-sun\n",
      "    Records:   728\n",
      "    Completed: 2025-11-02T13:50:12.537405\n",
      "\n",
      "  campuskey\n",
      "    Records:   61\n",
      "    Completed: 2025-11-02T13:50:21.236741\n",
      "\n",
      "============================================================\n",
      "LOG FILES\n",
      "============================================================\n",
      "  Execution Log:  pipeline_execution.log\n",
      "  Error Log:      pipeline_errors.log\n",
      "  Checkpoint:     pipeline_checkpoint.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Pipeline Execution Report\n",
    "Generate comprehensive report of the pipeline execution,\n",
    "including timing, success rates, and any failures.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE EXECUTION REPORT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Load checkpoint state\n",
    "report_data = checkpoint.state\n",
    "\n",
    "print(\"EXECUTION SUMMARY\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Start Time:              {report_data.get('start_time', 'N/A')}\")\n",
    "print(f\"Last Update:             {report_data.get('last_update', 'N/A')}\")\n",
    "print(f\"Total Records Processed: {report_data.get('total_records_processed', 0):,}\")\n",
    "print(f\"Total APIs:              {len(INSURANCE_APIS)}\")\n",
    "print(f\"Completed APIs:          {len(report_data.get('completed_apis', []))}\")\n",
    "print(f\"Failed APIs:             {len(report_data.get('failed_apis', []))}\")\n",
    "\n",
    "if report_data.get('completed_apis'):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"COMPLETED APIS\")\n",
    "    print(\"=\"*60)\n",
    "    for api in report_data['completed_apis']:\n",
    "        print(f\"\\n  {api['name']}\")\n",
    "        print(f\"    Records:   {api['records']:,}\")\n",
    "        print(f\"    Completed: {api['timestamp']}\")\n",
    "\n",
    "if report_data.get('failed_apis'):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FAILED APIS (Check error log for details)\")\n",
    "    print(\"=\"*60)\n",
    "    for api in report_data['failed_apis']:\n",
    "        print(f\"\\n  {api['name']}\")\n",
    "        print(f\"    Error:     {api['error']}\")\n",
    "        print(f\"    Timestamp: {api['timestamp']}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LOG FILES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Execution Log:  {CONFIG['LOG_FILE']}\")\n",
    "print(f\"  Error Log:      {CONFIG['ERROR_LOG']}\")\n",
    "print(f\"  Checkpoint:     {CONFIG['CHECKPOINT_FILE']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04cb694",
   "metadata": {},
   "source": [
    "## Cleanup Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1811129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLEANUP UTILITIES LOADED\n",
      "============================================================\n",
      "\n",
      "Available functions:\n",
      "  cleanup_temp_files()  - Remove temporary JSON chunks\n",
      "  reset_pipeline()      - Reset checkpoint and start over\n",
      "\n",
      "  Only use these after verifying your final CSV!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Cleanup Utilities\n",
    "Optional functions to clean up temporary files after successful completion.\n",
    "ONLY run these after verifying your final CSV is correct!\n",
    "\"\"\"\n",
    "\n",
    "def cleanup_temp_files():\n",
    "    \"\"\"\n",
    "    Remove all temporary chunk files after successful consolidation\n",
    "    WARNING: Only run this after verifying final output!\n",
    "    \"\"\"\n",
    "    temp_dir = Path(CONFIG['TEMP_DIR'])\n",
    "    \n",
    "    response = input(\"\\n  This will DELETE all temporary files. Are you sure? (yes/no): \")\n",
    "    if response.lower() != 'yes':\n",
    "        print(\"Cleanup cancelled.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        json_files = list(temp_dir.glob(\"*.json\"))\n",
    "        print(f\"\\nFound {len(json_files)} temporary files to delete...\")\n",
    "        \n",
    "        deleted_count = 0\n",
    "        for file in json_files:\n",
    "            try:\n",
    "                file.unlink()\n",
    "                deleted_count += 1\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to delete {file}: {e}\")\n",
    "        \n",
    "        print(f\" Deleted {deleted_count} temporary files\")\n",
    "        \n",
    "        # Optionally remove temp directory if empty\n",
    "        if not any(temp_dir.iterdir()):\n",
    "            temp_dir.rmdir()\n",
    "            print(f\" Removed empty directory: {temp_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Cleanup failed: {e}\")\n",
    "\n",
    "def reset_pipeline():\n",
    "    \"\"\"\n",
    "    Reset the entire pipeline (checkpoint and temp files)\n",
    "    WARNING: This will force a complete restart!\n",
    "    \"\"\"\n",
    "    response = input(\"\\n  This will RESET the entire pipeline. Are you sure? (yes/no): \")\n",
    "    if response.lower() != 'yes':\n",
    "        print(\"Reset cancelled.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Reset checkpoint\n",
    "        checkpoint.reset()\n",
    "        print(\" Checkpoint reset\")\n",
    "        \n",
    "        # Clean temp files\n",
    "        temp_dir = Path(CONFIG['TEMP_DIR'])\n",
    "        for file in temp_dir.glob(\"*.json\"):\n",
    "            file.unlink()\n",
    "        print(\" Temporary files deleted\")\n",
    "        \n",
    "        print(\"\\n Pipeline reset complete. You can now run from the beginning.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Reset failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLEANUP UTILITIES LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"  cleanup_temp_files()  - Remove temporary JSON chunks\")\n",
    "print(\"  reset_pipeline()      - Reset checkpoint and start over\")\n",
    "print(\"\\n  Only use these after verifying your final CSV!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc432ad",
   "metadata": {},
   "source": [
    "## Resume Pipeline (if interrupted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e5650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESUME FUNCTIONALITY\n",
      "============================================================\n",
      "\n",
      "To resume an interrupted pipeline, run:\n",
      "  await resume_pipeline()\n",
      "\n",
      "Current progress: 17/17 APIs completed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Resume Pipeline\n",
    "If the pipeline was interrupted, use this cell to resume from where it left off.\n",
    "The checkpoint system will automatically start from the last completed API.\n",
    "\"\"\"\n",
    "\n",
    "async def resume_pipeline():\n",
    "    \"\"\"\n",
    "    Resume pipeline execution from last checkpoint\n",
    "    \"\"\"\n",
    "    resume_index = checkpoint.get_resume_index()\n",
    "    \n",
    "    if resume_index >= len(INSURANCE_APIS):\n",
    "        print(\" All APIs have been processed!\")\n",
    "        print(\"  Run the consolidation cell if you haven't already.\")\n",
    "        return\n",
    "    \n",
    "    remaining = len(INSURANCE_APIS) - resume_index\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESUMING PIPELINE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Completed:  {resume_index} APIs\")\n",
    "    print(f\"  Remaining:  {remaining} APIs\")\n",
    "    print(f\"  Starting from: {INSURANCE_APIS[resume_index][1]}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    response = input(\"Continue? (yes/no): \")\n",
    "    if response.lower() != 'yes':\n",
    "        print(\"Resume cancelled.\")\n",
    "        return\n",
    "    \n",
    "    # Run the pipeline (it will automatically resume)\n",
    "    await run_pipeline()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESUME FUNCTIONALITY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTo resume an interrupted pipeline, run:\")\n",
    "print(\"  await resume_pipeline()\")\n",
    "print(f\"\\nCurrent progress: {checkpoint.get_resume_index()}/{len(INSURANCE_APIS)} APIs completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd5c56",
   "metadata": {},
   "source": [
    "## Memory Usage Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861be021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MEMORY USAGE\n",
      "============================================================\n",
      "  Process Memory:      95.63 MB\n",
      "  System Memory Used:  7196.93 MB / 8025.80 MB\n",
      "  System Memory:       89.7% used\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Memory Usage Monitor\n",
    "Track memory usage during execution to ensure we stay within limits.\n",
    "Useful for debugging and optimization.\n",
    "\"\"\"\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage statistics\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    \n",
    "    # Get system memory\n",
    "    system_memory = psutil.virtual_memory()\n",
    "    \n",
    "    return {\n",
    "        'process_mb': memory_info.rss / (1024 * 1024),\n",
    "        'system_total_mb': system_memory.total / (1024 * 1024),\n",
    "        'system_used_mb': system_memory.used / (1024 * 1024),\n",
    "        'system_percent': system_memory.percent\n",
    "    }\n",
    "\n",
    "def print_memory_status():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    try:\n",
    "        mem = get_memory_usage()\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"MEMORY USAGE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  Process Memory:      {mem['process_mb']:.2f} MB\")\n",
    "        print(f\"  System Memory Used:  {mem['system_used_mb']:.2f} MB / {mem['system_total_mb']:.2f} MB\")\n",
    "        print(f\"  System Memory:       {mem['system_percent']:.1f}% used\")\n",
    "        print(\"=\"*60)\n",
    "    except ImportError:\n",
    "        print(\"  psutil not installed. Run: pip install psutil\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error getting memory info: {e}\")\n",
    "\n",
    "# Show current memory usage\n",
    "print_memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf37508",
   "metadata": {},
   "source": [
    "## Advanced Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c430f301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analytics function loaded. Run with:\n",
      "  analyze_dataset()\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Quick Data Analytics\n",
    "Perform quick analytics on the collected data without loading everything into memory.\n",
    "\"\"\"\n",
    "\n",
    "def analyze_dataset():\n",
    "    \"\"\"\n",
    "    Perform quick analytics on the final dataset\n",
    "    Uses chunked processing for memory efficiency\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATASET ANALYTICS\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    if not Path(CONFIG['FINAL_OUTPUT']).exists():\n",
    "        print(f\" Dataset not found at {CONFIG['FINAL_OUTPUT']}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Initialize counters\n",
    "        total_records = 0\n",
    "        business_stats = {}\n",
    "        \n",
    "        # Process in chunks\n",
    "        print(\"Processing data in chunks...\")\n",
    "        for chunk in pd.read_csv(CONFIG['FINAL_OUTPUT'], chunksize=5000):\n",
    "            total_records += len(chunk)\n",
    "            \n",
    "            # Count by business\n",
    "            for business, count in chunk['Business_Name'].value_counts().items():\n",
    "                business_stats[business] = business_stats.get(business, 0) + count\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"OVERVIEW\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  Total Reviews:       {total_records:,}\")\n",
    "        print(f\"  Unique Businesses:   {len(business_stats)}\")\n",
    "        print(f\"  Average per Business: {total_records / len(business_stats):.0f}\")\n",
    "        \n",
    "        # Top and bottom performers\n",
    "        sorted_businesses = sorted(business_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"TOP 10 BUSINESSES (Most Reviews)\")\n",
    "        print(\"=\"*60)\n",
    "        for i, (business, count) in enumerate(sorted_businesses[:10], 1):\n",
    "            pct = (count / total_records) * 100\n",
    "            print(f\"  {i:2d}. {business:<35} {count:>8,} ({pct:>5.2f}%)\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"BOTTOM 10 BUSINESSES (Least Reviews)\")\n",
    "        print(\"=\"*60)\n",
    "        for i, (business, count) in enumerate(sorted_businesses[-10:], 1):\n",
    "            pct = (count / total_records) * 100\n",
    "            print(f\"  {i:2d}. {business:<35} {count:>8,} ({pct:>5.2f}%)\")\n",
    "        \n",
    "        # File size\n",
    "        file_size_mb = Path(CONFIG['FINAL_OUTPUT']).stat().st_size / (1024 * 1024)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"FILE INFORMATION\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  File Size:     {file_size_mb:.2f} MB\")\n",
    "        print(f\"  Average KB per record: {(file_size_mb * 1024) / total_records:.2f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error during analysis: {e}\")\n",
    "        logger.error(f\"Analytics failed: {e}\")\n",
    "\n",
    "print(\"\\nAnalytics function loaded. Run with:\")\n",
    "print(\"  analyze_dataset()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422cdb46",
   "metadata": {},
   "source": [
    "## Final Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81552456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PIPELINE SUMMARY\n",
      "============================================================\n",
      "\n",
      "OUTPUT FILES:\n",
      "------------------------------------------------------------\n",
      "   Final Dataset        customer_travel_reviews_final.csv   (17.73 MB)\n",
      "   Execution Log        pipeline_execution.log              (0.56 MB)\n",
      "   Error Log            pipeline_errors.log                 (0.02 MB)\n",
      "   Checkpoint           pipeline_checkpoint.json            (0.00 MB)\n",
      "   Temp Directory       temp_data                           (17917 files)\n",
      "\n",
      "============================================================\n",
      "NEXT STEPS\n",
      "============================================================\n",
      "\n",
      "1. Verify  data:\n",
      "   - Check the final CSV file\n",
      "   - Review the execution and error logs\n",
      "   - Run analytics: analyze_dataset()\n",
      "\n",
      "2. Clean up (optional):\n",
      "   - Remove temp files: cleanup_temp_files()\n",
      "   - Reset for fresh run: reset_pipeline()\n",
      "\n",
      "3. Export to other formats (optional):\n",
      "   - export_to_format('json')\n",
      "   - export_to_format('parquet')\n",
      "\n",
      "4. If interrupted:\n",
      "   - Resume from checkpoint: await resume_pipeline()\n",
      "\n",
      "5. Memory monitoring:\n",
      "   - Check usage: print_memory_status()\n",
      "\n",
      "============================================================\n",
      "Thank you for using the Data Collection Pipeline!\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Pipeline Complete! \n",
    "\n",
    "data collection pipeline has been successfully set up and executed.\n",
    "\n",
    "### What Was Accomplished:\n",
    "-  Collected reviews from multiple insurance company APIs\n",
    "-  Implemented fault-tolerant retry logic with exponential backoff\n",
    "-  Used asynchronous requests for efficient network I/O\n",
    "-  Saved data incrementally to minimize memory usage\n",
    "-  Created checkpoint system for resumable execution\n",
    "-  Consolidated all data into a single CSV file\n",
    "\n",
    "### Output Files:\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# List all output files\n",
    "output_files = {\n",
    "    'Final Dataset': CONFIG['FINAL_OUTPUT'],\n",
    "    'Execution Log': CONFIG['LOG_FILE'],\n",
    "    'Error Log': CONFIG['ERROR_LOG'],\n",
    "    'Checkpoint': CONFIG['CHECKPOINT_FILE'],\n",
    "    'Temp Directory': CONFIG['TEMP_DIR']\n",
    "}\n",
    "\n",
    "print(\"OUTPUT FILES:\")\n",
    "print(\"-\" * 60)\n",
    "for name, path in output_files.items():\n",
    "    file_path = Path(path)\n",
    "    if file_path.exists():\n",
    "        if file_path.is_file():\n",
    "            size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"   {name:<20} {path:<35} ({size_mb:.2f} MB)\")\n",
    "        else:\n",
    "            file_count = len(list(file_path.glob('*')))\n",
    "            print(f\"   {name:<20} {path:<35} ({file_count} files)\")\n",
    "    else:\n",
    "        print(f\"   {name:<20} {path:<35} (not found)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "1. Verify  data:\n",
    "   - Check the final CSV file\n",
    "   - Review the execution and error logs\n",
    "   - Run analytics: analyze_dataset()\n",
    "\n",
    "2. Clean up (optional):\n",
    "   - Remove temp files: cleanup_temp_files()\n",
    "   - Reset for fresh run: reset_pipeline()\n",
    "\n",
    "3. Export to other formats (optional):\n",
    "   - export_to_format('json')\n",
    "   - export_to_format('parquet')\n",
    "\n",
    "4. If interrupted:\n",
    "   - Resume from checkpoint: await resume_pipeline()\n",
    "\n",
    "5. Memory monitoring:\n",
    "   - Check usage: print_memory_status()\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Thank you for using the Data Collection Pipeline!\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
