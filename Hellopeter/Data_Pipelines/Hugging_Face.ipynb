{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce23502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset  # Use load_dataset for memory efficiency\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. Setup and Login ---\n",
    "\n",
    "# This loads the variables from your .env file\n",
    "load_dotenv()\n",
    "my_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if my_token:\n",
    "    print(\"Logging into Hugging Face Hub...\")\n",
    "    login(token=my_token)\n",
    "else:\n",
    "    print(\"ERROR: HF_TOKEN not found in .env file.\")\n",
    "    # You might want to exit the script if the token is missing\n",
    "    # exit() \n",
    "\n",
    "# --- 2. Load and Clean Pandas DataFrame ---\n",
    "\n",
    "print(\"Loading 'customer_insurance_reviews_final.csv'...\")\n",
    "final_complaints_df = pd.read_csv('customer_insurance_reviews_final.csv')\n",
    "\n",
    "print(\"Cleaning DataFrame...\")\n",
    "\n",
    "# Drop 'authorAvatar' column\n",
    "if 'authorAvatar' in final_complaints_df.columns:\n",
    "    final_complaints_df = final_complaints_df.drop(columns=['authorAvatar'])\n",
    "\n",
    "# Convert 'source_id' to string to prevent mixed-type errors\n",
    "if 'source_id' in final_complaints_df.columns:\n",
    "    final_complaints_df['source_id'] = final_complaints_df['source_id'].astype(str)\n",
    "\n",
    "# Convert 'incident_id_number' to string to prevent mixed-type errors\n",
    "if 'incident_id_number' in final_complaints_df.columns:\n",
    "    final_complaints_df['incident_id_number'] = final_complaints_df['incident_id_number'].astype(str)\n",
    "\n",
    "# --- (THIS IS THE NEW FIX) ---\n",
    "# Convert 'incident_contact_number' to string to fix the new ArrowTypeError\n",
    "if 'incident_contact_number' in final_complaints_df.columns:\n",
    "    final_complaints_df['incident_contact_number'] = final_complaints_df['incident_contact_number'].astype(str)\n",
    "# -----------------------------\n",
    "\n",
    "print(\"DataFrame Info after cleaning:\")\n",
    "final_complaints_df.info()\n",
    "\n",
    "# --- 3. Save to Parquet (The Memory-Safe Step) ---\n",
    "\n",
    "parquet_path = 'complaints.parquet'\n",
    "print(f\"Saving cleaned DataFrame to '{parquet_path}'...\")\n",
    "# This line should now work without error\n",
    "final_complaints_df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "# --- 4. Load Dataset from Parquet (Fix for ArrowMemoryError) ---\n",
    "\n",
    "print(f\"Loading dataset from '{parquet_path}'...\")\n",
    "# This loads the dataset efficiently from disk, not all into RAM\n",
    "dataset = load_dataset('parquet', data_files=parquet_path, split='train')\n",
    "\n",
    "# --- 5. Push to Hub ---\n",
    "\n",
    "repo_id = \"miehleketo93/customer_insurance_reviews\"\n",
    "print(f\"Pushing dataset to '{repo_id}'...\")\n",
    "dataset.push_to_hub(repo_id)\n",
    "\n",
    "print(\"\\n--- Process Complete ---\")\n",
    "print(f\"Dataset successfully pushed to https://huggingface.co/datasets/{repo_id}\")\n",
    "\n",
    "# Display the head of the final Hugging Face Dataset\n",
    "print(\"\\nDataset head:\")\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2af07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from huggingface_hub import login, HfApi\n",
    "from datasets import load_dataset, Features, Value\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. Setup and Login ---\n",
    "load_dotenv()\n",
    "my_token = os.getenv(\"HF_TOKEN\")\n",
    "HF_USERNAME = \"miehleketo93\" \n",
    "\n",
    "if my_token:\n",
    "    print(\"Logging into Hugging Face Hub...\")\n",
    "    login(token=my_token)\n",
    "else:\n",
    "    print(\"ERROR: HF_TOKEN not found in .env file.\")\n",
    "\n",
    "# Initialize the Hub API\n",
    "api = HfApi()\n",
    "\n",
    "# --- 2. DEFINE DATA FOLDER ---\n",
    "# Note the 'r' before the string to handle the backslashes\n",
    "DATA_DIRECTORY = r\"D:\\Data Engineering\\Data-Engineering\\Hellopeter\\Data_Pipelines\\csv_files\"\n",
    "\n",
    "# --- 3. Find and Loop Through All CSV Files ---\n",
    "csv_files = glob.glob(os.path.join(DATA_DIRECTORY, \"*.csv\"))\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"No .csv files found in '{DATA_DIRECTORY}'. Please check the path.\")\n",
    "else:\n",
    "    print(f\"Found {len(csv_files)} CSV files to process...\")\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    try:\n",
    "        # --- 4. Create Repo ID ---\n",
    "        # Get the filename without the .csv extension\n",
    "        dataset_name = os.path.basename(csv_path).replace('.csv', '')\n",
    "        \n",
    "        # Create the full repository ID\n",
    "        repo_id = f\"{HF_USERNAME}/{dataset_name}\"\n",
    "        \n",
    "        print(f\"\\n--- Processing: {dataset_name} ---\")\n",
    "        print(f\"Target repository: {repo_id}\")\n",
    "\n",
    "        # --- 5. Create Repo (if it doesn't exist) ---\n",
    "        api.create_repo(\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\",\n",
    "            exist_ok=True  \n",
    "        )\n",
    "        print(f\"Ensured repository exists: {repo_id}\")\n",
    "\n",
    "        # --- 6. Load CSV Dataset ---\n",
    "        print(f\"Loading data from {csv_path}...\")\n",
    "        dataset = load_dataset('csv', data_files=csv_path, split='train')\n",
    "\n",
    "        print(\"Casting known problematic columns to string...\")\n",
    "        new_features = dataset.features.copy()\n",
    "        columns_to_cast = ['source_id', 'incident_id_number', 'incident_contact_number']\n",
    "        \n",
    "        for col in columns_to_cast:\n",
    "            if col in new_features:\n",
    "                new_features[col] = Value('string')\n",
    "                \n",
    "        dataset = dataset.cast(new_features)\n",
    "        print(\"Casting complete.\")\n",
    "\n",
    "        # --- 8. Push to Hub ---\n",
    "        print(f\"Pushing data to {repo_id}...\")\n",
    "        dataset.push_to_hub(repo_id)\n",
    "        \n",
    "        print(f\" Successfully pushed '{dataset_name}' to the Hub.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" ERROR processing {csv_path}: {e}\")\n",
    "        print(\"Moving to next file...\")\n",
    "\n",
    "print(\"\\n--- All files processed. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3975884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from huggingface_hub import login, HfApi\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. Setup and Login ---\n",
    "load_dotenv()\n",
    "my_token = os.getenv(\"HF_TOKEN\")\n",
    "HF_USERNAME = \"miehleketo93\" \n",
    "\n",
    "if my_token:\n",
    "    print(\"Logging into Hugging Face Hub...\")\n",
    "    login(token=my_token)\n",
    "else:\n",
    "    print(\"ERROR: HF_TOKEN not found in .env file.\")\n",
    "    exit(1)\n",
    "\n",
    "# Initialize the Hub API\n",
    "api = HfApi()\n",
    "\n",
    "# --- 2. DELETE ALL EXISTING REPOS ---\n",
    "print(\"\\n=== DELETING ALL EXISTING DATASET REPOSITORIES ===\")\n",
    "try:\n",
    "    # List all datasets for the user\n",
    "    user_datasets = api.list_datasets(author=HF_USERNAME)\n",
    "    \n",
    "    dataset_list = list(user_datasets)\n",
    "    \n",
    "    if not dataset_list:\n",
    "        print(f\"No datasets found for user '{HF_USERNAME}'\")\n",
    "    else:\n",
    "        print(f\"Found {len(dataset_list)} datasets to delete...\")\n",
    "        \n",
    "        for dataset in dataset_list:\n",
    "            try:\n",
    "                repo_id = dataset.id\n",
    "                print(f\"Deleting: {repo_id}\")\n",
    "                api.delete_repo(repo_id=repo_id, repo_type=\"dataset\", token=my_token)\n",
    "                print(f\"✓ Deleted: {repo_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error deleting {repo_id}: {e}\")\n",
    "        \n",
    "        print(\"\\n✓ All repositories deleted successfully!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error listing/deleting repositories: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- 3. DEFINE DATA FOLDER ---\n",
    "DATA_DIRECTORY = r\"D:\\Data Engineering\\Data-Engineering\\Hellopeter\\Data_Pipelines\\csv_files\"\n",
    "\n",
    "# --- 4. Find CSV Files ---\n",
    "csv_files = glob.glob(os.path.join(DATA_DIRECTORY, \"*.csv\"))\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"\\nNo .csv files found in '{DATA_DIRECTORY}'. Please check the path.\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(f\"\\n=== UPLOADING {len(csv_files)} DATASETS ===\")\n",
    "\n",
    "# --- 5. Process Each CSV File ---\n",
    "for csv_path in csv_files:\n",
    "    try:\n",
    "        # Get dataset name from filename\n",
    "        dataset_name = os.path.basename(csv_path).replace('.csv', '')\n",
    "        repo_id = f\"{HF_USERNAME}/{dataset_name}\"\n",
    "        \n",
    "        print(f\"\\n--- Processing: {dataset_name} ---\")\n",
    "        \n",
    "        # Generate description based on dataset name\n",
    "        description = f\"\"\"# {dataset_name.replace('_', ' ').title()} Dataset\n",
    "\n",
    "This dataset contains {dataset_name.replace('_', ' ')} data from HelloPeter.\n",
    "\n",
    "## Dataset Information\n",
    "- **Format**: Parquet (optimized for fast loading)\n",
    "- **Source**: HelloPeter Data Pipeline\n",
    "- **Split**: train\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"{repo_id}\")\n",
    "```\n",
    "\n",
    "---\n",
    "*This dataset is part of the HelloPeter data collection.*\n",
    "\"\"\"\n",
    "        \n",
    "        # --- 6. Load CSV with Robust Parsing ---\n",
    "        print(f\"Loading data from {csv_path}...\")\n",
    "        \n",
    "        # Try multiple parsing strategies\n",
    "        df = None\n",
    "        parsing_strategies = [\n",
    "            # Strategy 1: Standard with error handling\n",
    "            {\n",
    "                'on_bad_lines': 'skip',\n",
    "                'engine': 'python',\n",
    "                'quoting': 1,  # QUOTE_ALL\n",
    "                'escapechar': '\\\\'\n",
    "            },\n",
    "            # Strategy 2: More lenient\n",
    "            {\n",
    "                'on_bad_lines': 'skip',\n",
    "                'engine': 'python',\n",
    "                'quotechar': '\"',\n",
    "                'doublequote': True,\n",
    "                'escapechar': None\n",
    "            },\n",
    "            # Strategy 3: C engine with error skip\n",
    "            {\n",
    "                'on_bad_lines': 'skip',\n",
    "                'engine': 'c',\n",
    "                'quoting': 1\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for idx, strategy in enumerate(parsing_strategies, 1):\n",
    "            try:\n",
    "                print(f\"  Attempting parsing strategy {idx}...\")\n",
    "                df = pd.read_csv(csv_path, **strategy, low_memory=False)\n",
    "                print(f\"  ✓ Successfully parsed with strategy {idx}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Strategy {idx} failed: {str(e)[:100]}\")\n",
    "                if idx == len(parsing_strategies):\n",
    "                    raise Exception(\"All parsing strategies failed\")\n",
    "                continue\n",
    "        \n",
    "        if df is None or df.empty:\n",
    "            raise Exception(\"Failed to load data or dataframe is empty\")\n",
    "        \n",
    "        print(f\"✓ Loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
    "        \n",
    "        # --- 7. Convert to Dataset ---\n",
    "        dataset = Dataset.from_pandas(df)\n",
    "        \n",
    "        # Cast problematic columns to string if they exist\n",
    "        columns_to_cast = ['source_id', 'incident_id_number', 'incident_contact_number']\n",
    "        \n",
    "        for col in columns_to_cast:\n",
    "            if col in dataset.column_names:\n",
    "                # Convert column to string\n",
    "                dataset = dataset.map(\n",
    "                    lambda x: {col: str(x[col]) if x[col] is not None else ''},\n",
    "                    desc=f\"Converting {col} to string\"\n",
    "                )\n",
    "        \n",
    "        print(f\"Dataset shape: {dataset.num_rows} rows, {dataset.num_columns} columns\")\n",
    "        \n",
    "        # --- 8. Create Private Repository ---\n",
    "        print(f\"Creating private repository: {repo_id}...\")\n",
    "        api.create_repo(\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\",\n",
    "            private=True,\n",
    "            exist_ok=True\n",
    "        )\n",
    "        \n",
    "        # --- 9. Push to Hub in Parquet Format ---\n",
    "        print(f\"Pushing dataset to Hub (Parquet format)...\")\n",
    "        dataset.push_to_hub(\n",
    "            repo_id=repo_id,\n",
    "            token=my_token,\n",
    "            private=True\n",
    "        )\n",
    "        \n",
    "        # --- 10. Update Repository Description ---\n",
    "        print(\"Updating repository description...\")\n",
    "        api.update_repo_settings(\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\",\n",
    "            private=True,\n",
    "            description=description\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Successfully uploaded '{dataset_name}' as private Parquet dataset\")\n",
    "        print(f\"  URL: https://huggingface.co/datasets/{repo_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ ERROR processing {csv_path}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"\\nContinuing to next file...\\n\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✓ ALL DATASETS PROCESSED!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
